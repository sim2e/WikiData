In statistics, the precision matrix or concentration matrix is the matrix inverse of the covariance matrix or dispersion matrix, 
  
    
      
        P
        =
        
          Σ
          
            −
            1
          
        
      
    
    {\displaystyle P=\Sigma ^{-1}}
  .
For univariate distributions, the precision matrix degenerates into a scalar precision, defined as the reciprocal of the variance, 
  
    
      
        p
        =
        
          
            1
            
              σ
              
                2
              
            
          
        
      
    
    {\displaystyle p={\frac {1}{\sigma ^{2}}}}
  .Other summary statistics of statistical dispersion also called precision (or imprecision)
include the reciprocal of the standard deviation, 
  
    
      
        p
        =
        
          
            1
            σ
          
        
      
    
    {\displaystyle p={\frac {1}{\sigma }}}
  ; 
the standard deviation itself and the relative standard deviation;
as well as the standard error and the confidence interval (or its half-width, the margin of error).

Usage
One particular use of the precision matrix is in the context of Bayesian analysis of the multivariate normal distribution: for example, Bernardo & Smith prefer to parameterise the multivariate normal distribution in terms of the precision matrix, rather than the covariance matrix, because of certain simplifications that then arise. For instance, if both the prior and the likelihood have Gaussian form, and the precision matrix of both of these exist (because their covariance matrix is full rank and thus invertible), then the precision matrix of the posterior will simply be the sum of the precision matrices of the prior and the likelihood.
As the inverse of a Hermitian matrix, the precision matrix of real-valued random variables, if it exists, is positive definite and symmetrical.
Another reason the precision matrix may be useful is that if two dimensions 
  
    
      
        i
      
    
    {\displaystyle i}
   and 
  
    
      
        j
      
    
    {\displaystyle j}
   of a multivariate normal are conditionally independent, then the 
  
    
      
        i
        j
      
    
    {\displaystyle ij}
   and 
  
    
      
        j
        i
      
    
    {\displaystyle ji}
   elements of the precision matrix are 
  
    
      
        0
      
    
    {\displaystyle 0}
  . This means that precision matrices tend to be sparse when many of the dimensions are conditionally independent, which can lead to computational efficiencies when working with them. It also means that precision matrices are closely related to the idea of partial correlation.
The precision matrix plays a central role in generalized least squares, compared to ordinary least squares, where 
  
    
      
        P
      
    
    {\displaystyle P}
   is the identity matrix, and to weighted least squares, where 
  
    
      
        P
      
    
    {\displaystyle P}
   is diagonal (the weight matrix).

Etymology
The term precision in this sense ("mensura praecisionis observationum") first appeared in the works of Gauss (1809) "Theoria motus corporum coelestium in sectionibus conicis solem ambientium" (page 212). Gauss's definition differs from the modern one by a factor of 
  
    
      
        
          
            2
          
        
      
    
    {\displaystyle {\sqrt {2}}}
  . He writes, for the density function of a normal distribution with precision 
  
    
      
        h
      
    
    {\displaystyle h}
   (reciprocal of standard deviation),

  
    
      
        φ
        Δ
        =
        
          
            h
            
              π
            
          
        
        
        
          e
          
            −
            h
            h
            Δ
            Δ
          
        
        .
      
    
    {\displaystyle \varphi \Delta ={\frac {h}{\sqrt {\pi }}}\,e^{-hh\Delta \Delta }.}
  where 
  
    
      
        h
        h
        =
        
          h
          
            2
          
        
      
    
    {\displaystyle hh=h^{2}}
   (see: Exponentiation#History of the notation).
Later Whittaker & Robinson (1924) "Calculus of observations" called this quantity the modulus (of precision), but this term has dropped out of use.


== References ==