In mathematics, the exterior algebra of a vector space V is a graded associative algebra, 

  
    
      
        ∧
        V
        =
        k
        ⊕
        V
        ⊕
        
          ∧
          
            2
          
        
        V
        ⊕
        
          ∧
          
            3
          
        
        V
        ⊕
        ⋯
      
    
    {\displaystyle \wedge V=k\oplus V\oplus \wedge ^{2}V\oplus \wedge ^{3}V\oplus \cdots }
  The corresponding product ∧ on the exterior algebra ∧V is called the exterior product or wedge product. Every element of ∧V is a sum of elements v1∧ ... ∧ vk, where 
  
    
      
        
          v
          
            i
          
        
        ∈
        V
      
    
    {\displaystyle v_{i}\in V}
  . An element of this form is called a k-blade; it corresponds to the oriented parallelotope spanned by them. 

Elements in ∧kV are called k-multivectors, and are given by a sum of k-blades; it is an abstraction of oriented lengths, areas, volumes and more generally oriented k-volumes for k ≥ 0. 
The algebra's exterior product ∧ satisfies the alternating property v∧v = 0  for all 
  
    
      
        v
        ∈
        V
      
    
    {\displaystyle v\in V}
  . By distributivity and linearity, this also implies it is antisymmetric: u∧v = - v∧u . More generally, any blade flips sign whenever two adjacent vectors are exchanged. The negative of any k-blade corresponds to a parallelotope with opposite orientation. 

The exterior algebra is also called the Grassmann algebra, after Hermann Grassmann.

Formal definitions
There are several equivalent ways to define the exterior algebra of a vector space V.

Definition as a quotient of the tensor algebra
Let V be a vector space over a field. 
Recall the tensor algebra TV. As a vector space it is spanned by symbols v1⊗ ... ⊗vk for k ≥ 0 and vi ∈ V, and the only relations are those specifying these objects be linear in each variable vi.

Definition. The exterior algebra  is the quotient 

  
    
      
        ∧
        V
         
        =
         
        T
        V
        
          /
        
        ⟨
        u
        ⊗
        v
        +
        v
        ⊗
        u
         
        :
         
        u
        ,
        v
        ∈
        V
        ⟩
      
    
    {\displaystyle \wedge V\ =\ TV/\langle u\otimes v+v\otimes u\ :\ u,v\in V\rangle }
  by the two-sided ideal generated by elements of the form u⊗v + v⊗u. Denote by 

v1∧ ... ∧ vkthe class of v1⊗ ... ⊗vk in the quotient ∧V.

∧V inherits the structure of a graded algebra from TV. An element of ∧V may be written (non-uniquely) as a finite sum

v1∧ ... ∧ vk1+ w1∧ ... ∧ wk2 + ... + z1∧ ... ∧ zknwhere each letter is an element of V, and ki, n ≥ 0. It is called a  k-vector if all ki = k, in which case its rank is the minimum n for which it can be written in the above form. Rank one k-vectors

v1∧ ... ∧ vkare also called  k-blades.

Definition in terms of explicit basis
Let e1, ..., en be a basis of V. 

Definition. The exterior algebra ∧V is the vector space with basis formal symbols
ei1∧ ... ∧ eiawhere a ≥ 0 and i1 < ... < ia is a strictly increasing sequence. The exterior product is 

(ei1∧ ... ∧ eia) ∧ (ej1∧ ... ∧ ejb) = (0 or ± ek1∧ ... ∧ eka+b)where in the first case the sequences i and j have an element in common. If they do not, k is formed by taking i1 ... ia j1 ... jb and permuting elements until the sequence is increasing, each swap of two adjacent elements flipping the sign ±.

Motivating examples
The first two examples assume a metric tensor field and an orientation; the third example does not assume either.

Areas in the plane
The Cartesian plane 
  
    
      
        
          
            R
          
          
            2
          
        
      
    
    {\displaystyle \mathbb {R} ^{2}}
   is a real vector space equipped with a basis consisting of a pair of unit vectors

  
    
      
        
          
            
              e
            
          
          
            1
          
        
        =
        
          
            [
            
              
                
                  1
                
              
              
                
                  0
                
              
            
            ]
          
        
        ,
        
        
          
            
              e
            
          
          
            2
          
        
        =
        
          
            [
            
              
                
                  0
                
              
              
                
                  1
                
              
            
            ]
          
        
        ,
      
    
    {\displaystyle {\mathbf {e} }_{1}={\begin{bmatrix}1\\0\end{bmatrix}},\quad {\mathbf {e} }_{2}={\begin{bmatrix}0\\1\end{bmatrix}},}
   with the orientation 
  
    
      
        
          
            e
          
          
            1
          
        
        ×
        
          
            e
          
          
            2
          
        
      
    
    {\displaystyle \mathbf {e} _{1}\times \mathbf {e} _{2}}
   and with the metric 
  
    
      
        
          
            [
            
              
                
                  1
                
                
                  0
                
              
              
                
                  0
                
                
                  1
                
              
            
            ]
          
        
        .
      
    
    {\displaystyle {\begin{bmatrix}1&0\\0&1\end{bmatrix}}.}
  Suppose that

  
    
      
        
          v
        
        =
        
          
            [
            
              
                
                  a
                
              
              
                
                  b
                
              
            
            ]
          
        
        =
        a
        
          
            e
          
          
            1
          
        
        +
        b
        
          
            e
          
          
            2
          
        
        ,
        
        
          w
        
        =
        
          
            [
            
              
                
                  c
                
              
              
                
                  d
                
              
            
            ]
          
        
        =
        c
        
          
            e
          
          
            1
          
        
        +
        d
        
          
            e
          
          
            2
          
        
      
    
    {\displaystyle \mathbf {v} ={\begin{bmatrix}a\\b\end{bmatrix}}=a\mathbf {e} _{1}+b\mathbf {e} _{2},\quad \mathbf {w} ={\begin{bmatrix}c\\d\end{bmatrix}}=c\mathbf {e} _{1}+d\mathbf {e} _{2}}
  are a pair of given vectors in 
  
    
      
        
          
            R
          
          
            2
          
        
        ,
      
    
    {\displaystyle \mathbb {R} ^{2},}
   written in components.  There is a unique parallelogram having v and w as two of its sides.  The area of this parallelogram is given by the standard determinant formula:

  
    
      
        
          Area
        
        =
        
          
            |
          
        
        det
        
          
            [
            
              
                
                  
                    v
                  
                
                
                  
                    w
                  
                
              
            
            ]
          
        
        
          
            |
          
        
        =
        
          
            |
          
        
        det
        
          
            [
            
              
                
                  a
                
                
                  c
                
              
              
                
                  b
                
                
                  d
                
              
            
            ]
          
        
        
          
            |
          
        
        =
        
          |
          
            a
            d
            −
            b
            c
          
          |
        
        .
      
    
    {\displaystyle {\text{Area}}={\Bigl |}\det {\begin{bmatrix}\mathbf {v} &\mathbf {w} \end{bmatrix}}{\Bigr |}={\Biggl |}\det {\begin{bmatrix}a&c\\b&d\end{bmatrix}}{\Biggr |}=\left|ad-bc\right|.}
  Consider now the exterior product of v and w:

  
    
      
        
          
            
              
                
                  v
                
                ∧
                
                  w
                
              
              
                
                =
                (
                a
                
                  
                    e
                  
                  
                    1
                  
                
                +
                b
                
                  
                    e
                  
                  
                    2
                  
                
                )
                ∧
                (
                c
                
                  
                    e
                  
                  
                    1
                  
                
                +
                d
                
                  
                    e
                  
                  
                    2
                  
                
                )
              
            
            
              
              
                
                =
                a
                c
                
                  
                    e
                  
                  
                    1
                  
                
                ∧
                
                  
                    e
                  
                  
                    1
                  
                
                +
                a
                d
                
                  
                    e
                  
                  
                    1
                  
                
                ∧
                
                  
                    e
                  
                  
                    2
                  
                
                +
                b
                c
                
                  
                    e
                  
                  
                    2
                  
                
                ∧
                
                  
                    e
                  
                  
                    1
                  
                
                +
                b
                d
                
                  
                    e
                  
                  
                    2
                  
                
                ∧
                
                  
                    e
                  
                  
                    2
                  
                
              
            
            
              
              
                
                =
                
                  (
                  
                    a
                    d
                    −
                    b
                    c
                  
                  )
                
                
                  
                    e
                  
                  
                    1
                  
                
                ∧
                
                  
                    e
                  
                  
                    2
                  
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}\mathbf {v} \wedge \mathbf {w} &=(a\mathbf {e} _{1}+b\mathbf {e} _{2})\wedge (c\mathbf {e} _{1}+d\mathbf {e} _{2})\\&=ac\mathbf {e} _{1}\wedge \mathbf {e} _{1}+ad\mathbf {e} _{1}\wedge \mathbf {e} _{2}+bc\mathbf {e} _{2}\wedge \mathbf {e} _{1}+bd\mathbf {e} _{2}\wedge \mathbf {e} _{2}\\&=\left(ad-bc\right)\mathbf {e} _{1}\wedge \mathbf {e} _{2}\end{aligned}}}
  where the first step uses the distributive law for the exterior product, and the last uses the fact that the exterior product is an alternating map, and in particular 
  
    
      
        
          
            e
          
          
            2
          
        
        ∧
        
          
            e
          
          
            1
          
        
        =
        −
        (
        
          
            e
          
          
            1
          
        
        ∧
        
          
            e
          
          
            2
          
        
        )
        .
      
    
    {\displaystyle \mathbf {e} _{2}\wedge \mathbf {e} _{1}=-(\mathbf {e} _{1}\wedge \mathbf {e} _{2}).}
    (The fact that the exterior product is an alternating map also forces 
  
    
      
        
          
            e
          
          
            1
          
        
        ∧
        
          
            e
          
          
            1
          
        
        =
        
          
            e
          
          
            2
          
        
        ∧
        
          
            e
          
          
            2
          
        
        =
        0.
      
    
    {\displaystyle \mathbf {e} _{1}\wedge \mathbf {e} _{1}=\mathbf {e} _{2}\wedge \mathbf {e} _{2}=0.}
  ) Note that the coefficient in this last expression is precisely the determinant of the matrix [v w].  The fact that this may be positive or negative has the intuitive meaning that v and w may be oriented in a counterclockwise or clockwise sense as the vertices of the parallelogram they define.  Such an area is called the signed area of the parallelogram: the absolute value of the signed area is the ordinary area, and the sign determines its orientation.
The fact that this coefficient is the signed area is not an accident.  In fact, it is relatively easy to see that the exterior product should be related to the signed area if one tries to axiomatize this area as an algebraic construct.  In detail, if A(v, w) denotes the signed area of the parallelogram of which the pair of vectors v and w form two adjacent sides, then A must satisfy the following properties:

A(rv, sw) = rsA(v, w) for any real numbers r and s, since rescaling either of the sides rescales the area by the same amount (and reversing the direction of one of the sides reverses the orientation of the parallelogram).
A(v, v) = 0, since the area of the degenerate parallelogram determined by v (i.e., a line segment) is zero.
A(w, v) = −A(v, w), since interchanging the roles of v and w reverses the orientation of the parallelogram.
A(v + rw, w) = A(v, w) for any real number r, since adding a multiple of w to v affects neither the base nor the height of the parallelogram and consequently preserves its area.
A(e1, e2) = 1, since the area of the unit square is one.With the exception of the last property, the exterior product of two vectors satisfies the same properties as the area.  In a certain sense, the exterior product generalizes the final property by allowing the area of a parallelogram to be compared to that of any chosen parallelogram in a parallel plane (here, the one with sides e1 and e2).  In other words, the exterior product provides a basis-independent formulation of area.

Oriented areas in space
The power of linearity of the cross product, implicitly, of the exterior product, is seen in a simple geometric property, not that obvious: the total signed/oriented area of a non self overlapping polyhedron
is zero. This is just the simplest linear algebra, if using exterior product, but quite a lot of work, using metric geometry: 

  
    
      
        (
        
          u
        
        −
        
          v
        
        )
        ∧
        (
        
          w
        
        −
        
          v
        
        )
        +
        
          u
        
        ∧
        
          v
        
        +
        
          w
        
        ∧
        
          u
        
        +
        
          v
        
        ∧
        
          w
        
        =
        0
      
    
    {\displaystyle (\mathbf {u} -\mathbf {v} )\wedge (\mathbf {w} -\mathbf {v} )+\mathbf {u} \wedge \mathbf {v} +\mathbf {w} \wedge \mathbf {u} +\mathbf {v} \wedge \mathbf {w} =0}
  Each blade in the sum above is (twice) the oriented (either all inward, or all outward) surface area of the four faces of a tetrahedron with concurrent edges along vectors 
  
    
      
        
          u
        
        ,
        
          v
        
        ,
        
          w
        
      
    
    {\displaystyle \mathbf {u} ,\mathbf {v} ,\mathbf {w} }
  . Iterating this property for each and all tetrahedrons that build a non self overlapping polyhedron, we get the general statement.

Oriented volume in affine space
The natural setting for (oriented) 
  
    
      
        k
      
    
    {\displaystyle k}
  -dimensional volume and exterior algebra is affine space. This is also the intimate connection between exterior algebra and differential forms, as to integrate we need a 'differential' object to measure infinitesimal volume. If 
  
    
      
        
          A
        
      
    
    {\displaystyle \mathbb {A} }
   is an affine space over the vector space 
  
    
      
        V
      
    
    {\displaystyle V}
  , and a (simplex) collection of ordered 
  
    
      
        k
        +
        1
      
    
    {\displaystyle k+1}
   points 
  
    
      
        
          A
          
            0
          
        
        ,
        
          A
          
            1
          
        
        ,
        .
        .
        .
        ,
        
          A
          
            k
          
        
      
    
    {\displaystyle A_{0},A_{1},...,A_{k}}
  , we can define its oriented 
  
    
      
        k
      
    
    {\displaystyle k}
  -dimensional volume as the exterior product of vectors 
  
    
      
        
          A
          
            0
          
        
        
          A
          
            1
          
        
        ∧
        
          A
          
            0
          
        
        
          A
          
            2
          
        
        ∧
        ⋯
        ∧
        
          A
          
            0
          
        
        
          A
          
            k
          
        
        =
        

        
      
    
    {\displaystyle A_{0}A_{1}\wedge A_{0}A_{2}\wedge \cdots \wedge A_{0}A_{k}={}}
   
  
    
      
        (
        −
        1
        
          )
          
            j
          
        
        
          A
          
            j
          
        
        
          A
          
            0
          
        
        ∧
        
          A
          
            j
          
        
        
          A
          
            1
          
        
        ∧
        
          A
          
            j
          
        
        
          A
          
            2
          
        
        ∧
        ⋯
        ∧
        
          A
          
            j
          
        
        
          A
          
            k
          
        
      
    
    {\displaystyle (-1)^{j}A_{j}A_{0}\wedge A_{j}A_{1}\wedge A_{j}A_{2}\wedge \cdots \wedge A_{j}A_{k}}
   (using concatenation 
  
    
      
        P
        Q
      
    
    {\displaystyle PQ}
   to mean the displacement vector from point 
  
    
      
        P
      
    
    {\displaystyle P}
   to 
  
    
      
        Q
      
    
    {\displaystyle Q}
  ); if the order of the points is changed, the oriented volume changes by a sign, according to the parity of the permutation. In 
  
    
      
        n
      
    
    {\displaystyle n}
  -dimensional space, the volume of any 
  
    
      
        n
      
    
    {\displaystyle n}
  -dimensional simplex is a scalar multiple of any other.
The sum of the 
  
    
      
        (
        k
        −
        1
        )
      
    
    {\displaystyle (k-1)}
  -dimensional oriented areas of the boundary simplexes of a 
  
    
      
        k
      
    
    {\displaystyle k}
  -dimensional simplex is zero, as for the sum of vectors around a triangle or the oriented triangles bounding the tetrahedron in the previous section. This remains true if the simplex is "flat" (is contained in an 
  
    
      
        (
        k
        −
        1
        )
      
    
    {\displaystyle (k-1)}
  -dimensional affine subspace, as 4 points in a plane).

The vector space structure on ∧V generalises addition of vectors in V: we have (u1+u2)∧v = u1∧v + u2∧v  and similarly a k-blade v1∧ ... ∧ vk is linear in each factor.

Cross and triple products
Take a 3-dimensional vector space, e.g. R3, and choose a basis (e1, e2, e3). The exterior product of a pair of vectors

  
    
      
        
          u
        
        =
        
          u
          
            1
          
        
        
          
            e
          
          
            1
          
        
        +
        
          u
          
            2
          
        
        
          
            e
          
          
            2
          
        
        +
        
          u
          
            3
          
        
        
          
            e
          
          
            3
          
        
        
         
         
         
        
         
         
         
        
        
           and 
        
        
         
         
         
        
         
         
         
        
         
         
         
        
          v
        
        =
        
          v
          
            1
          
        
        
          
            e
          
          
            1
          
        
        +
        
          v
          
            2
          
        
        
          
            e
          
          
            2
          
        
        +
        
          v
          
            3
          
        
        
          
            e
          
          
            3
          
        
      
    
    {\displaystyle \mathbf {u} =u_{1}\mathbf {e} _{1}+u_{2}\mathbf {e} _{2}+u_{3}\mathbf {e} _{3}\,\ \ \ \,\ \ \ \,{\text{ and }}\,\ \ \ \,\ \ \ \,\ \ \ \mathbf {v} =v_{1}\mathbf {e} _{1}+v_{2}\mathbf {e} _{2}+v_{3}\mathbf {e} _{3}}
  is

  
    
      
        
          u
        
        ∧
        
          v
        
        =
        (
        
          u
          
            1
          
        
        
          v
          
            2
          
        
        −
        
          u
          
            2
          
        
        
          v
          
            1
          
        
        )
        (
        
          
            e
          
          
            1
          
        
        ∧
        
          
            e
          
          
            2
          
        
        )
        +
        (
        
          u
          
            2
          
        
        
          v
          
            3
          
        
        −
        
          u
          
            3
          
        
        
          v
          
            2
          
        
        )
        (
        
          
            e
          
          
            2
          
        
        ∧
        
          
            e
          
          
            3
          
        
        )
        +
        (
        
          u
          
            3
          
        
        
          v
          
            1
          
        
        −
        
          u
          
            1
          
        
        
          v
          
            3
          
        
        )
        (
        
          
            e
          
          
            3
          
        
        ∧
        
          
            e
          
          
            1
          
        
        )
        .
      
    
    {\displaystyle \mathbf {u} \wedge \mathbf {v} =(u_{1}v_{2}-u_{2}v_{1})(\mathbf {e} _{1}\wedge \mathbf {e} _{2})+(u_{2}v_{3}-u_{3}v_{2})(\mathbf {e} _{2}\wedge \mathbf {e} _{3})+(u_{3}v_{1}-u_{1}v_{3})(\mathbf {e} _{3}\wedge \mathbf {e} _{1}).}
  If we use the basis (e1 ∧ e2, e2 ∧ e3, e3 ∧ e1) to identify ∧2R3 ≅ R3, this becomes the usual formula for cross product of the two vectors.
Bringing in a third vector

  
    
      
        
          w
        
        =
        
          w
          
            1
          
        
        
          
            e
          
          
            1
          
        
        +
        
          w
          
            2
          
        
        
          
            e
          
          
            2
          
        
        +
        
          w
          
            3
          
        
        
          
            e
          
          
            3
          
        
        ,
      
    
    {\displaystyle \mathbf {w} =w_{1}\mathbf {e} _{1}+w_{2}\mathbf {e} _{2}+w_{3}\mathbf {e} _{3},}
  the exterior product of three vectors is

  
    
      
        
          u
        
        ∧
        
          v
        
        ∧
        
          w
        
        =
        (
        
          u
          
            1
          
        
        
          v
          
            2
          
        
        
          w
          
            3
          
        
        +
        
          u
          
            2
          
        
        
          v
          
            3
          
        
        
          w
          
            1
          
        
        +
        
          u
          
            3
          
        
        
          v
          
            1
          
        
        
          w
          
            2
          
        
        −
        
          u
          
            1
          
        
        
          v
          
            3
          
        
        
          w
          
            2
          
        
        −
        
          u
          
            2
          
        
        
          v
          
            1
          
        
        
          w
          
            3
          
        
        −
        
          u
          
            3
          
        
        
          v
          
            2
          
        
        
          w
          
            1
          
        
        )
        (
        
          
            e
          
          
            1
          
        
        ∧
        
          
            e
          
          
            2
          
        
        ∧
        
          
            e
          
          
            3
          
        
        )
        .
      
    
    {\displaystyle \mathbf {u} \wedge \mathbf {v} \wedge \mathbf {w} =(u_{1}v_{2}w_{3}+u_{2}v_{3}w_{1}+u_{3}v_{1}w_{2}-u_{1}v_{3}w_{2}-u_{2}v_{1}w_{3}-u_{3}v_{2}w_{1})(\mathbf {e} _{1}\wedge \mathbf {e} _{2}\wedge \mathbf {e} _{3}).}
  Using the basis e1 ∧ e2 ∧ e3 to identify ∧2R3 ≅ R, this becomes the usual formula for triple product of the three vectors.

Electromagnetic field
In Einstein's theories of relativity, the electromagnetic field is generally given as a differential 2-form 
  
    
      
        F
        =
        d
        A
      
    
    {\displaystyle F=dA}
   in 4-space or as the equivalent alternating tensor field 
  
    
      
        
          F
          
            i
            j
          
        
        =
        
          A
          
            [
            i
            ,
            j
            ]
          
        
        =
        
          A
          
            [
            i
            ;
            j
            ]
          
        
        ,
      
    
    {\displaystyle F_{ij}=A_{[i,j]}=A_{[i;j]},}
   the electromagnetic tensor.  Then 
  
    
      
        d
        F
        =
        d
        d
        A
        =
        0
      
    
    {\displaystyle dF=ddA=0}
   or the equivalent Bianchi identity 
  
    
      
        
          F
          
            [
            i
            j
            ,
            k
            ]
          
        
        =
        
          F
          
            [
            i
            j
            ;
            k
            ]
          
        
        =
        0.
      
    
    {\displaystyle F_{[ij,k]}=F_{[ij;k]}=0.}
  
None of this requires a metric.
Adding the Lorentz metric and an orientation provides the Hodge star operator 
  
    
      
        ⋆
      
    
    {\displaystyle \star }
   and thus makes it possible to define 
  
    
      
        J
        =
        
          ⋆
        
        d
        
          ⋆
        
        F
      
    
    {\displaystyle J={\star }d{\star }F}
   or the equivalent tensor divergence 
  
    
      
        
          J
          
            i
          
        
        =
        
          F
          
            ,
            j
          
          
            i
            j
          
        
        =
        
          F
          
            ;
            j
          
          
            i
            j
          
        
      
    
    {\displaystyle J^{i}=F_{,j}^{ij}=F_{;j}^{ij}}
   where 
  
    
      
        
          F
          
            i
            j
          
        
        =
        
          g
          
            i
            k
          
        
        
          g
          
            j
            l
          
        
        
          F
          
            k
            l
          
        
        .
      
    
    {\displaystyle F^{ij}=g^{ik}g^{jl}F_{kl}.}

Plücker embedding
The wedge product was introduced originally as an algebraic construction to study k-dimensional subspaces of Rn, which forms a manifold Gr(k,n) called the Grassmannian. It lives inside the projectivisation of the space of multivectors

  
    
      
        
          Gr
        
        (
        k
        ,
        n
        )
         
        ⊆
         
        
          P
        
        (
        
          ∧
          
            k
          
        
        
          
            R
          
          
            n
          
        
        )
        .
      
    
    {\displaystyle {\text{Gr}}(k,n)\ \subseteq \ \mathbf {P} (\wedge ^{k}\mathbf {R} ^{n}).}
  This map is called the Plücker embedding, and its image is defined by the Plücker relations.

Differential forms
The definition of the exterior algebra can be extended to any module over a commutative ring, or more generally a sheaf of modules over a ringed space. For instance, differential forms over a manifold X

  
    
      
        
          ∧
          
            
              
                
                  O
                
              
              
                X
              
            
          
        
        
          Ω
          
            X
          
          
            1
          
        
      
    
    {\displaystyle \wedge _{{\mathcal {O}}_{X}}\Omega _{X}^{1}}
  is the exterior algebra on the sheaf of one-forms ΩX, which is a sheaf of modules over the sheaf of functions OX.

Algebraic properties
Alternating product
The exterior product is by construction alternating on elements of 
  
    
      
        V
        ,
      
    
    {\displaystyle V,}
   which means that 
  
    
      
        x
        ∧
        x
        =
        0
      
    
    {\textstyle x\wedge x=0}
   for all 
  
    
      
        x
        ∈
        V
        ,
      
    
    {\displaystyle x\in V,}
   by the above construction.  It follows that the product is also anticommutative on elements of 
  
    
      
        V
        ,
      
    
    {\displaystyle V,}
   for supposing that 
  
    
      
        x
        ,
        y
        ∈
        V
        ,
      
    
    {\displaystyle x,y\in V,}
  

  
    
      
        0
        =
        (
        x
        +
        y
        )
        ∧
        (
        x
        +
        y
        )
        =
        x
        ∧
        x
        +
        x
        ∧
        y
        +
        y
        ∧
        x
        +
        y
        ∧
        y
        =
        x
        ∧
        y
        +
        y
        ∧
        x
      
    
    {\displaystyle 0=(x+y)\wedge (x+y)=x\wedge x+x\wedge y+y\wedge x+y\wedge y=x\wedge y+y\wedge x}
  hence

  
    
      
        x
        ∧
        y
        =
        −
        (
        y
        ∧
        x
        )
        .
      
    
    {\displaystyle x\wedge y=-(y\wedge x).}
  More generally, if σ is a permutation of the integers [1, ..., k], and x1, x2, ..., xk are elements of V, it follows that

  
    
      
        
          x
          
            σ
            (
            1
            )
          
        
        ∧
        
          x
          
            σ
            (
            2
            )
          
        
        ∧
        ⋯
        ∧
        
          x
          
            σ
            (
            k
            )
          
        
        =
        sgn
        ⁡
        (
        σ
        )
        
          x
          
            1
          
        
        ∧
        
          x
          
            2
          
        
        ∧
        ⋯
        ∧
        
          x
          
            k
          
        
        ,
      
    
    {\displaystyle x_{\sigma (1)}\wedge x_{\sigma (2)}\wedge \cdots \wedge x_{\sigma (k)}=\operatorname {sgn} (\sigma )x_{1}\wedge x_{2}\wedge \cdots \wedge x_{k},}
  where sgn(σ) is the signature of the permutation σ.In particular, if xi = xj for some i ≠ j, then the following generalization of the alternating property also holds:

  
    
      
        
          x
          
            1
          
        
        ∧
        
          x
          
            2
          
        
        ∧
        ⋯
        ∧
        
          x
          
            k
          
        
        =
        0.
      
    
    {\displaystyle x_{1}\wedge x_{2}\wedge \cdots \wedge x_{k}=0.}
  Together with the distributive property of the exterior product, one further generalization is that if and only if 
  
    
      
        {
        
          x
          
            1
          
        
        ,
        
          x
          
            2
          
        
        ,
        ⋯
        ,
        
          x
          
            k
          
        
        }
      
    
    {\displaystyle \{x_{1},x_{2},\cdots ,x_{k}\}}
   is a linearly dependent set of vectors, then 

  
    
      
        
          x
          
            1
          
        
        ∧
        
          x
          
            2
          
        
        ∧
        ⋯
        ∧
        
          x
          
            k
          
        
        =
        0.
      
    
    {\displaystyle x_{1}\wedge x_{2}\wedge \cdots \wedge x_{k}=0.}

Exterior power
The kth exterior power of V, denoted 
  
    
      
        
          ⋀
          
            k
          
        
        (
        V
        )
        ,
      
    
    {\textstyle \bigwedge \nolimits ^{k}(V),}
   is the vector subspace of 
  
    
      
        ⋀
        (
        V
        )
      
    
    {\textstyle \bigwedge (V)}
   spanned by elements of the form

  
    
      
        
          x
          
            1
          
        
        ∧
        
          x
          
            2
          
        
        ∧
        ⋯
        ∧
        
          x
          
            k
          
        
        ,
        
        
          x
          
            i
          
        
        ∈
        V
        ,
        i
        =
        1
        ,
        2
        ,
        …
        ,
        k
        .
      
    
    {\displaystyle x_{1}\wedge x_{2}\wedge \cdots \wedge x_{k},\quad x_{i}\in V,i=1,2,\ldots ,k.}
  If 
  
    
      
        α
        ∈
        
          ⋀
          
            k
          
        
        (
        V
        )
        ,
      
    
    {\textstyle \alpha \in \bigwedge \nolimits ^{k}(V),}
   then α is said to be a k-vector.  If, furthermore, α can be expressed as an exterior product of k elements of V, then α is said to be decomposable (or simple, by some authors; or a blade, by others).  Although decomposable k-vectors span 
  
    
      
        
          ⋀
          
            k
          
        
        (
        V
        )
        ,
      
    
    {\textstyle \bigwedge \nolimits ^{k}(V),}
   not every element of 
  
    
      
        
          ⋀
          
            k
          
        
        (
        V
        )
      
    
    {\textstyle \bigwedge \nolimits ^{k}(V)}
   is decomposable.  For example, in 
  
    
      
        
          
            R
          
          
            4
          
        
        ,
      
    
    {\displaystyle \mathbb {R} ^{4},}
   the following 2-vector is not decomposable:

  
    
      
        α
        =
        
          e
          
            1
          
        
        ∧
        
          e
          
            2
          
        
        +
        
          e
          
            3
          
        
        ∧
        
          e
          
            4
          
        
        .
      
    
    {\displaystyle \alpha =e_{1}\wedge e_{2}+e_{3}\wedge e_{4}.}
  (This is a symplectic form, since α ∧ α ≠ 0.). Note that being decomposable is crucial property in projective geometry and it is the outcome of the Plücker constraints (also see Plücker coordinates ).

Basis and dimension
If the dimension of V is n and  { e1, …, en }  is a basis for V, then the set

  
    
      
        {
        
        
          e
          
            
              i
              
                1
              
            
          
        
        ∧
        
          e
          
            
              i
              
                2
              
            
          
        
        ∧
        ⋯
        ∧
        
          e
          
            
              i
              
                k
              
            
          
        
         
        
          
            |
          
        
         
         
        1
        ≤
        
          i
          
            1
          
        
        <
        
          i
          
            2
          
        
        <
        ⋯
        <
        
          i
          
            k
          
        
        ≤
        n
        
        }
      
    
    {\displaystyle \{\,e_{i_{1}}\wedge e_{i_{2}}\wedge \cdots \wedge e_{i_{k}}~{\big |}~~1\leq i_{1}<i_{2}<\cdots <i_{k}\leq n\,\}}
  is a basis for 
  
    
      
        
          ⋀
          
            k
          
        
        (
        V
        )
        .
      
    
    {\textstyle \bigwedge \nolimits ^{k}(V).}
    The reason is the following: given any exterior product of the form

  
    
      
        
          v
          
            1
          
        
        ∧
        ⋯
        ∧
        
          v
          
            k
          
        
        ,
      
    
    {\displaystyle v_{1}\wedge \cdots \wedge v_{k},}
  every vector vj can be written as a linear combination of the basis vectors ei; using the bilinearity of the exterior product, this can be expanded to a linear combination of exterior products of those basis vectors.  Any exterior product in which the same basis vector appears more than once is zero; any exterior product in which the basis vectors do not appear in the proper order can be reordered, changing the sign whenever two basis vectors change places.  In general, the resulting coefficients of the basis k-vectors can be computed as the minors of the matrix that describes the vectors vj in terms of the basis ei.
By counting the basis elements, the dimension of 
  
    
      
        
          ⋀
          
            k
          
        
        (
        V
        )
      
    
    {\textstyle \bigwedge \nolimits ^{k}(V)}
   is equal to a binomial coefficient:

  
    
      
        dim
        ⁡
        
          
            
              ⋀
            
          
          
            k
          
        
        (
        V
        )
        =
        
          
            
              (
            
            
              n
              k
            
            
              )
            
          
        
        
      
    
    {\displaystyle \dim {\textstyle \bigwedge }^{k}(V)={\binom {n}{k}}\,}
  where n is the dimension of the vectors, and k is the number of vectors in the product.  The binomial coefficient produces the correct result, even for exceptional cases; in particular, 
  
    
      
        
          ⋀
          
            k
          
        
        (
        V
        )
        =
        {
        0
        }
      
    
    {\textstyle \bigwedge \nolimits ^{k}(V)=\{0\}}
   for  k > n.
Any element of the exterior algebra can be written as a sum of k-vectors.  Hence, as a vector space the exterior algebra is a direct sum

  
    
      
        
          
            ⋀
          
        
        (
        V
        )
        =
        
          
            
              ⋀
            
          
          
            0
          
        
        (
        V
        )
        ⊕
        
          
            
              ⋀
            
          
          
            1
          
        
        (
        V
        )
        ⊕
        
          
            
              ⋀
            
          
          
            2
          
        
        (
        V
        )
        ⊕
        ⋯
        ⊕
        
          
            
              ⋀
            
          
          
            n
          
        
        (
        V
        )
      
    
    {\displaystyle {\textstyle \bigwedge }(V)={\textstyle \bigwedge }^{0}(V)\oplus {\textstyle \bigwedge }^{1}(V)\oplus {\textstyle \bigwedge }^{2}(V)\oplus \cdots \oplus {\textstyle \bigwedge }^{n}(V)}
  (where by convention 
  
    
      
        
          ⋀
          
            0
          
        
        (
        V
        )
        =
        K
        ,
      
    
    {\textstyle \bigwedge \nolimits ^{0}(V)=K,}
   the field underlying V, and 
  
    
      
        
          ⋀
          
            1
          
        
        (
        V
        )
        =
        V
      
    
    {\textstyle \bigwedge \nolimits ^{1}(V)=V}
  ), and therefore its dimension is equal to the sum of the binomial coefficients, which is 2n.

Rank of a k-vector
If 
  
    
      
        α
        ∈
        
          ⋀
          
            k
          
        
        (
        V
        )
        ,
      
    
    {\textstyle \alpha \in \bigwedge \nolimits ^{k}(V),}
   then it is possible to express α as a linear combination of decomposable k-vectors:

  
    
      
        α
        =
        
          α
          
            (
            1
            )
          
        
        +
        
          α
          
            (
            2
            )
          
        
        +
        ⋯
        +
        
          α
          
            (
            s
            )
          
        
      
    
    {\displaystyle \alpha =\alpha ^{(1)}+\alpha ^{(2)}+\cdots +\alpha ^{(s)}}
  where each α(i) is decomposable, say

  
    
      
        
          α
          
            (
            i
            )
          
        
        =
        
          α
          
            1
          
          
            (
            i
            )
          
        
        ∧
        ⋯
        ∧
        
          α
          
            k
          
          
            (
            i
            )
          
        
        ,
        
        i
        =
        1
        ,
        2
        ,
        …
        ,
        s
        .
      
    
    {\displaystyle \alpha ^{(i)}=\alpha _{1}^{(i)}\wedge \cdots \wedge \alpha _{k}^{(i)},\quad i=1,2,\ldots ,s.}
  The rank of the k-vector α is the minimal number of decomposable k-vectors in such an expansion of α.  This is similar to the notion of tensor rank.
Rank is particularly important in the study of 2-vectors (Sternberg 1964, §III.6) (Bryant et al. 1991).  The rank of a 2-vector α can be identified with half the rank of the matrix of coefficients of α in a basis.  Thus if ei is a basis for V, then α can be expressed uniquely as

  
    
      
        α
        =
        
          ∑
          
            i
            ,
            j
          
        
        
          a
          
            i
            j
          
        
        
          e
          
            i
          
        
        ∧
        
          e
          
            j
          
        
      
    
    {\displaystyle \alpha =\sum _{i,j}a_{ij}e_{i}\wedge e_{j}}
  where aij = −aji (the matrix of coefficients is skew-symmetric).  The rank of the matrix aij is therefore even, and is twice the rank of the form α.
In characteristic 0, the 2-vector α has rank p if and only if

  
    
      
        
          
            
              
                
                  α
                  ∧
                  ⋯
                  ∧
                  α
                
                ⏟
              
            
            p
          
        
        ≠
        0
         
      
    
    {\displaystyle {\underset {p}{\underbrace {\alpha \wedge \cdots \wedge \alpha } }}\neq 0\ }
   and 
  
    
      
         
        
          
            
              
                
                  α
                  ∧
                  ⋯
                  ∧
                  α
                
                ⏟
              
            
            
              p
              +
              1
            
          
        
        =
        0.
      
    
    {\displaystyle \ {\underset {p+1}{\underbrace {\alpha \wedge \cdots \wedge \alpha } }}=0.}

Graded structure
The exterior product of a k-vector with a p-vector is a (k + p)-vector, once again invoking bilinearity.  As a consequence, the direct sum decomposition of the preceding section

  
    
      
        
          
            ⋀
          
        
        (
        V
        )
        =
        
          
            
              ⋀
            
          
          
            
            0
          
        
        (
        V
        )
        ⊕
        
          
            
              ⋀
            
          
          
            
            1
          
        
        (
        V
        )
        ⊕
        
          
            
              ⋀
            
          
          
            
            2
          
        
        (
        V
        )
        ⊕
        ⋯
        ⊕
        
          
            
              ⋀
            
          
          
            
            n
          
        
        (
        V
        )
      
    
    {\displaystyle {\textstyle \bigwedge }(V)={\textstyle \bigwedge }^{\!0}(V)\oplus {\textstyle \bigwedge }^{\!1}(V)\oplus {\textstyle \bigwedge }^{\!2}(V)\oplus \cdots \oplus {\textstyle \bigwedge }^{\!n}(V)}
  gives the exterior algebra the additional structure of a graded algebra, that is

  
    
      
        
          
            
              ⋀
            
          
          
            k
          
        
        (
        V
        )
        ∧
        
          
            
              ⋀
            
          
          
            p
          
        
        (
        V
        )
        ⊂
        
          
            
              ⋀
            
          
          
            k
            +
            p
          
        
        (
        V
        )
        .
      
    
    {\displaystyle {\textstyle \bigwedge }^{k}(V)\wedge {\textstyle \bigwedge }^{p}(V)\subset {\textstyle \bigwedge }^{k+p}(V).}
  Moreover, if K is the base field, we have

  
    
      
        
          
            
              ⋀
            
          
          
            
            0
          
        
        (
        V
        )
        =
        K
      
    
    {\displaystyle {\textstyle \bigwedge }^{\!0}(V)=K}
   and 
  
    
      
        
          
            
              ⋀
            
          
          
            
            1
          
        
        (
        V
        )
        =
        V
        .
      
    
    {\displaystyle {\textstyle \bigwedge }^{\!1}(V)=V.}
  The exterior product is graded anticommutative, meaning that if 
  
    
      
        α
        ∈
        
          ⋀
          
            k
          
        
        (
        V
        )
      
    
    {\textstyle \alpha \in \bigwedge \nolimits ^{k}(V)}
   and 
  
    
      
        β
        ∈
        
          ⋀
          
            p
          
        
        (
        V
        )
        ,
      
    
    {\textstyle \beta \in \bigwedge \nolimits ^{p}(V),}
   then

  
    
      
        α
        ∧
        β
        =
        (
        −
        1
        
          )
          
            k
            p
          
        
        β
        ∧
        α
        .
      
    
    {\displaystyle \alpha \wedge \beta =(-1)^{kp}\beta \wedge \alpha .}
  In addition to studying the graded structure on the exterior algebra, Bourbaki (1989) studies additional graded structures on exterior algebras, such as those on the exterior algebra of a graded module (a module that already carries its own gradation).

Universal property
Let V be a vector space over the field K.  Informally, multiplication in 
  
    
      
        ⋀
        (
        V
        )
      
    
    {\textstyle \bigwedge (V)}
   is performed by manipulating symbols and imposing a distributive law, an associative law, and using the identity 
  
    
      
        v
        ∧
        v
        =
        0
      
    
    {\displaystyle v\wedge v=0}
   for v ∈ V.  Formally, 
  
    
      
        ⋀
        (
        V
        )
      
    
    {\textstyle \bigwedge (V)}
   is the "most general" algebra in which these rules hold for the multiplication, in the sense that any unital associative K-algebra containing V with alternating multiplication on V must contain a homomorphic image of 
  
    
      
        ⋀
        (
        V
        )
        .
      
    
    {\textstyle \bigwedge (V).}
    In other words, the exterior algebra has the following universal property:

To construct the most general algebra that contains V and whose multiplication is alternating on V, it is natural to start with the most general associative algebra that contains V, the tensor algebra T(V), and then enforce the alternating property by taking a suitable quotient.  We thus take the two-sided ideal I in T(V) generated by all elements of the form v ⊗ v for v in V, and define 
  
    
      
        ⋀
        (
        V
        )
      
    
    {\textstyle \bigwedge (V)}
   as the quotient

  
    
      
        
          
            ⋀
          
        
        (
        V
        )
        =
        T
        (
        V
        )
        
          /
        
        I
         
      
    
    {\displaystyle {\textstyle \bigwedge }(V)=T(V)/I\ }
  (and use ∧ as the symbol for multiplication in 
  
    
      
        ⋀
        (
        V
        )
      
    
    {\textstyle \bigwedge (V)}
  ).  It is then straightforward to show that 
  
    
      
        ⋀
        (
        V
        )
      
    
    {\textstyle \bigwedge (V)}
   contains V and satisfies the above universal property.
As a consequence of this construction, the operation of assigning to a vector space V its exterior algebra 
  
    
      
        ⋀
        (
        V
        )
      
    
    {\textstyle \bigwedge (V)}
   is a functor from the category of vector spaces to the category of algebras.
Rather than defining 
  
    
      
        ⋀
        (
        V
        )
      
    
    {\textstyle \bigwedge (V)}
   first and then identifying the exterior powers 
  
    
      
        
          ⋀
          
            k
          
        
        (
        V
        )
      
    
    {\textstyle \bigwedge \nolimits ^{k}(V)}
   as certain subspaces, one may alternatively define the spaces 
  
    
      
        
          ⋀
          
            k
          
        
        (
        V
        )
      
    
    {\textstyle \bigwedge \nolimits ^{k}(V)}
   first and then combine them to form the algebra 
  
    
      
        ⋀
        (
        V
        )
        .
      
    
    {\textstyle \bigwedge (V).}
    This approach is often used in differential geometry and is described in the next section.

Generalizations
Given a commutative ring R and an R-module M, we can define the exterior algebra 
  
    
      
        ⋀
        (
        M
        )
      
    
    {\textstyle \bigwedge (M)}
   just as above, as a suitable quotient of the tensor algebra T(M).  It will satisfy the analogous universal property.  Many of the properties of 
  
    
      
        ⋀
        (
        M
        )
      
    
    {\textstyle \bigwedge (M)}
   also require that M be a projective module.  Where finite dimensionality is used, the properties further require that M be finitely generated and projective.  Generalizations to the most common situations can be found in Bourbaki (1989).
Exterior algebras of vector bundles are frequently considered in geometry and topology.  There are no essential differences between the algebraic properties of the exterior algebra of finite-dimensional vector bundles and those of the exterior algebra of finitely generated projective modules, by the Serre–Swan theorem.  More general exterior algebras can be defined for sheaves of modules.

Alternating tensor algebra
Regardless  the characteristic of the field (except characteristic 2), the exterior algebra of a vector space V over K can be canonically identified with the vector subspace of T(V) consisting of antisymmetric tensors. For characteristic 0 (or higher than the dimension of the vector space 
  
    
      
        V
      
    
    {\textstyle V}
  ), the vector space of 
  
    
      
        k
      
    
    {\displaystyle k}
  -linear antisymmetric tensors is transversal to the ideal  
  
    
      
        I
      
    
    {\textstyle I}
  , hence, a good choice to represent the quotient. But for nonzero characteristic, the vector space of k-linear antisymmetric tensors could be not transversal to the ideal (actually, for 
  
    
      
        k
        ≥
      
    
    {\textstyle k\geq }
   the characteristic of the field, the vector space of k-linear antisymmetric tensors is contained in 
  
    
      
        I
      
    
    {\textstyle I}
  ); nevertheless, transversal or not, a product can be defined on this space such that the resulting algebra is isomorphic to the exterior algebra: in the first case the natural choice for the product is just the quotient product (using the available projection), in the second case, this product must be slightly modified as given below (along Arnold setting), but such that the algebra stays isomorphic with the exterior algebra , i.e. the quotient of T(V) by the ideal I generated by elements of the form x ⊗ x. Of course, for characteristic 0 (or higher than the dimension of the vector space), one or the other definition of the product could be used, as the two algebras are isomorphic (see V. I. Arnold or Kobayashi-Nomizu). 
Let Tr(V) be the space of homogeneous tensors of degree r.  This is spanned by decomposable tensors

  
    
      
        
          v
          
            1
          
        
        ⊗
        ⋯
        ⊗
        
          v
          
            r
          
        
        ,
        
        
          v
          
            i
          
        
        ∈
        V
        .
      
    
    {\displaystyle v_{1}\otimes \cdots \otimes v_{r},\quad v_{i}\in V.}
  The antisymmetrization (or sometimes the skew-symmetrization) of a decomposable tensor is defined by

  
    
      
        
          
            
              
                A
                l
                t
              
            
            
              (
              r
              )
            
          
        
        ⁡
        (
        
          v
          
            1
          
        
        ⊗
        ⋯
        ⊗
        
          v
          
            r
          
        
        )
        =
        
          ∑
          
            σ
            ∈
            
              
                
                  S
                
              
              
                r
              
            
          
        
        sgn
        ⁡
        (
        σ
        )
        
          v
          
            σ
            (
            1
            )
          
        
        ⊗
        ⋯
        ⊗
        
          v
          
            σ
            (
            r
            )
          
        
      
    
    {\displaystyle \operatorname {{\mathcal {Alt}}^{(r)}} (v_{1}\otimes \cdots \otimes v_{r})=\sum _{\sigma \in {\mathfrak {S}}_{r}}\operatorname {sgn} (\sigma )v_{\sigma (1)}\otimes \cdots \otimes v_{\sigma (r)}}
  and, when 
  
    
      
        r
        !
        ≠
        0
      
    
    {\displaystyle r!\neq 0}
   (for nonzero characteristic field 
  
    
      
        r
        !
      
    
    {\displaystyle r!}
   might be 0):

  
    
      
        
          Alt
          
            (
            r
            )
          
        
        ⁡
        (
        
          v
          
            1
          
        
        ⊗
        ⋯
        ⊗
        
          v
          
            r
          
        
        )
        =
        
          
            1
            
              r
              !
            
          
        
        
          
            
              
                A
                l
                t
              
            
            
              (
              r
              )
            
          
        
        ⁡
        (
        
          v
          
            1
          
        
        ⊗
        ⋯
        ⊗
        
          v
          
            r
          
        
        )
      
    
    {\displaystyle \operatorname {Alt} ^{(r)}(v_{1}\otimes \cdots \otimes v_{r})={\frac {1}{r!}}\operatorname {{\mathcal {Alt}}^{(r)}} (v_{1}\otimes \cdots \otimes v_{r})}
  where the sum is taken over the symmetric group of permutations on the symbols {1, ..., r}. This extends by linearity and homogeneity to an operation, also denoted by 
  
    
      
        
          
            A
            l
            t
          
        
      
    
    {\displaystyle {\mathcal {Alt}}}
   and 
  
    
      
        
          
            A
            l
            t
          
        
      
    
    {\displaystyle {\rm {Alt}}}
  , on the full tensor algebra T(V).  
Note that: 

  
    
      
        
          
            
              
                A
                l
                t
              
            
            
              (
              r
              )
            
          
        
        ⁡
        
          
            
              
                A
                l
                t
              
            
            
              (
              r
              )
            
          
        
        =
        r
        !
        
          
            
              
                A
                l
                t
              
            
            
              (
              r
              )
            
          
        
        .
      
    
    {\displaystyle \operatorname {{\mathcal {Alt}}^{(r)}} \operatorname {{\mathcal {Alt}}^{(r)}} =r!\operatorname {{\mathcal {Alt}}^{(r)}} .}
  Such that, when defined, 
  
    
      
        
          Alt
          
            (
            r
            )
          
        
      
    
    {\displaystyle \operatorname {Alt} ^{(r)}}
   is the projection for the exterior (quotient) algebra onto the r-homogeneous alternating tensor subspace.
On the other hand, the image 
  
    
      
        
          
            A
            l
            t
          
        
      
    
    {\displaystyle {\mathcal {Alt}}}
  (T(V)) is always the alternating tensor graded subspace (not yet an algebra, as product is not yet defined), denoted 
  
    
      
        A
        (
        V
        )
      
    
    {\displaystyle A(V)}
  . This is a vector subspace of T(V), and it inherits the structure of a graded vector space from that on T(V). Moreover, the kernel of 
  
    
      
        
          
            
              A
              l
              t
            
          
          
            (
            r
            )
          
        
      
    
    {\displaystyle {\mathcal {Alt}}^{(r)}}
   is precisely I
  
    
      
        
          
          
            (
            r
            )
          
        
      
    
    {\displaystyle ^{(r)}}
  , the homogeneous subset of the ideal I, or the kernel of  
  
    
      
        
          
            A
            l
            t
          
        
      
    
    {\displaystyle {\mathcal {Alt}}}
   is I.  When 
  
    
      
        Alt
      
    
    {\displaystyle \operatorname {Alt} }
   is defined, 
  
    
      
        A
        (
        V
        )
      
    
    {\displaystyle A(V)}
   carries an associative graded product 
  
    
      
        
          
            
              ⊗
              ^
            
          
        
      
    
    {\displaystyle {\widehat {\otimes }}}
   defined by (the same as the wedge product)

  
    
      
        t
        ∧
        s
        =
        t
         
        
          
            
              ⊗
              ^
            
          
        
         
        s
        =
        Alt
        ⁡
        (
        t
        ⊗
        s
        )
        .
      
    
    {\displaystyle t\wedge s=t~{\widehat {\otimes }}~s=\operatorname {Alt} (t\otimes s).}
  Assuming K has characteristic 0, as 
  
    
      
        A
        (
        V
        )
      
    
    {\displaystyle A(V)}
   is a supplement of I in T(V), with the above given product, there is a canonical isomorphism

  
    
      
        A
        (
        V
        )
        ≅
        
          
            ⋀
          
        
        (
        V
        )
        .
      
    
    {\displaystyle A(V)\cong {\textstyle \bigwedge }(V).}
  When the characteristic of the field is nonzero, 
  
    
      
        
          
            A
            l
            t
          
        
      
    
    {\displaystyle {\mathcal {Alt}}}
   will do what 
  
    
      
        
          
            A
            l
            t
          
        
      
    
    {\displaystyle {\rm {Alt}}}
   did before, but the  product cannot be defined as above. In such a case, isomorphism 
  
    
      
        A
        (
        V
        )
        ≅
        
          
            ⋀
          
        
        (
        V
        )
      
    
    {\displaystyle A(V)\cong {\textstyle \bigwedge }(V)}
   still holds, in spite of 
  
    
      
        A
        (
        V
        )
      
    
    {\displaystyle A(V)}
   not being a supplement of  the ideal I, but then, the product should be modified as given below (
  
    
      
        
          
            
              ∧
              ˙
            
          
        
      
    
    {\displaystyle {\dot {\wedge }}}
   product , Arnold setting).
Finally, we always get  
  
    
      
        A
        (
        V
        )
      
    
    {\displaystyle A(V)}
   isomorphic with  
  
    
      
        
          
            ⋀
          
        
        (
        V
        )
      
    
    {\displaystyle {\textstyle \bigwedge }(V)}
  , but the product could (or should) be chosen in two ways (or only one). Actually, the product could be chosen in many ways, rescaling it on homogeneous spaces as 
  
    
      
        c
        (
        r
        +
        p
        )
        
          /
        
        c
        (
        r
        )
        c
        (
        p
        )
      
    
    {\displaystyle c(r+p)/c(r)c(p)}
   for an arbitrary sequence 
  
    
      
        c
        (
        r
        )
      
    
    {\displaystyle c(r)}
   in the field, as long as the division makes sense (this is such that the redefined product is also associative, i.e. defines an algebra on 
  
    
      
        A
        (
        V
        )
      
    
    {\displaystyle A(V)}
  ). Also note, the interior product definition should be changed accordingly, in order to keep its skew derivation property.

Index notation
Suppose that V has finite dimension n, and that a basis e1, ..., en of V is given.  Then any alternating tensor t ∈ Ar(V) ⊂ Tr(V) can be written in index notation as

  
    
      
        t
        =
        
          t
          
            
              i
              
                1
              
            
            
              i
              
                2
              
            
            ⋯
            
              i
              
                r
              
            
          
        
        
        
          
            
              e
            
          
          
            
              i
              
                1
              
            
          
        
        ⊗
        
          
            
              e
            
          
          
            
              i
              
                2
              
            
          
        
        ⊗
        ⋯
        ⊗
        
          
            
              e
            
          
          
            
              i
              
                r
              
            
          
        
        ,
      
    
    {\displaystyle t=t^{i_{1}i_{2}\cdots i_{r}}\,{\mathbf {e} }_{i_{1}}\otimes {\mathbf {e} }_{i_{2}}\otimes \cdots \otimes {\mathbf {e} }_{i_{r}},}
  where ti1⋅⋅⋅ir is completely antisymmetric in its indices.
The exterior product of two alternating tensors t and s of ranks r and p is given by

  
    
      
        t
         
        
          
            
              ⊗
              ^
            
          
        
         
        s
        =
        
          
            1
            
              (
              r
              +
              p
              )
              !
            
          
        
        
          ∑
          
            σ
            ∈
            
              
                
                  S
                
              
              
                r
                +
                p
              
            
          
        
        sgn
        ⁡
        (
        σ
        )
        
          t
          
            
              i
              
                σ
                (
                1
                )
              
            
            ⋯
            
              i
              
                σ
                (
                r
                )
              
            
          
        
        
          s
          
            
              i
              
                σ
                (
                r
                +
                1
                )
              
            
            ⋯
            
              i
              
                σ
                (
                r
                +
                p
                )
              
            
          
        
        
          
            
              e
            
          
          
            
              i
              
                1
              
            
          
        
        ⊗
        
          
            
              e
            
          
          
            
              i
              
                2
              
            
          
        
        ⊗
        ⋯
        ⊗
        
          
            
              e
            
          
          
            
              i
              
                r
                +
                p
              
            
          
        
        .
      
    
    {\displaystyle t~{\widehat {\otimes }}~s={\frac {1}{(r+p)!}}\sum _{\sigma \in {\mathfrak {S}}_{r+p}}\operatorname {sgn} (\sigma )t^{i_{\sigma (1)}\cdots i_{\sigma (r)}}s^{i_{\sigma (r+1)}\cdots i_{\sigma (r+p)}}{\mathbf {e} }_{i_{1}}\otimes {\mathbf {e} }_{i_{2}}\otimes \cdots \otimes {\mathbf {e} }_{i_{r+p}}.}
  The components of this tensor are precisely the skew part of the components of the tensor product s ⊗ t, denoted by square brackets on the indices:

  
    
      
        (
        t
         
        
          
            
              ⊗
              ^
            
          
        
         
        s
        
          )
          
            
              i
              
                1
              
            
            ⋯
            
              i
              
                r
                +
                p
              
            
          
        
        =
        
          t
          
            [
            
              i
              
                1
              
            
            ⋯
            
              i
              
                r
              
            
          
        
        
          s
          
            
              i
              
                r
                +
                1
              
            
            ⋯
            
              i
              
                r
                +
                p
              
            
            ]
          
        
        .
      
    
    {\displaystyle (t~{\widehat {\otimes }}~s)^{i_{1}\cdots i_{r+p}}=t^{[i_{1}\cdots i_{r}}s^{i_{r+1}\cdots i_{r+p}]}.}
  The interior product may also be described in index notation as follows.  Let 
  
    
      
        t
        =
        
          t
          
            
              i
              
                0
              
            
            
              i
              
                1
              
            
            ⋯
            
              i
              
                r
                −
                1
              
            
          
        
      
    
    {\displaystyle t=t^{i_{0}i_{1}\cdots i_{r-1}}}
   be an antisymmetric tensor of rank r.  Then, for α ∈ V∗, iαt is an alternating tensor of rank r − 1, given by

  
    
      
        (
        
          i
          
            α
          
        
        t
        
          )
          
            
              i
              
                1
              
            
            ⋯
            
              i
              
                r
                −
                1
              
            
          
        
        =
        r
        
          ∑
          
            j
            =
            0
          
          
            n
          
        
        
          α
          
            j
          
        
        
          t
          
            j
            
              i
              
                1
              
            
            ⋯
            
              i
              
                r
                −
                1
              
            
          
        
        .
      
    
    {\displaystyle (i_{\alpha }t)^{i_{1}\cdots i_{r-1}}=r\sum _{j=0}^{n}\alpha _{j}t^{ji_{1}\cdots i_{r-1}}.}
  where n is the dimension of V.

Duality
Alternating operators
Given two vector spaces V and X and a natural number k, an alternating operator from Vk to X is a multilinear map

  
    
      
        f
        :
        
          V
          
            k
          
        
        →
        X
      
    
    {\displaystyle f\colon V^{k}\to X}
  such that whenever v1, ..., vk are linearly dependent vectors in V, then

  
    
      
        f
        (
        
          v
          
            1
          
        
        ,
        …
        ,
        
          v
          
            k
          
        
        )
        =
        0.
      
    
    {\displaystyle f(v_{1},\ldots ,v_{k})=0.}
  The map

  
    
      
        w
        :
        
          V
          
            k
          
        
        →
        
          
            
              ⋀
            
          
          
            
            k
          
        
        (
        V
        )
        ,
      
    
    {\displaystyle w\colon V^{k}\to {\textstyle \bigwedge }^{\!k}(V),}
  which associates to 
  
    
      
        k
      
    
    {\displaystyle k}
   vectors from 
  
    
      
        V
      
    
    {\displaystyle V}
   their exterior product, i.e. their corresponding 
  
    
      
        k
      
    
    {\displaystyle k}
  -vector, is also alternating.  In fact, this map is the "most general" alternating operator defined on 
  
    
      
        
          V
          
            k
          
        
        ;
      
    
    {\displaystyle V^{k};}
   given any other alternating operator 
  
    
      
        f
        :
        
          V
          
            k
          
        
        →
        X
        ,
      
    
    {\displaystyle f:V^{k}\rightarrow X,}
   there exists a unique linear map 
  
    
      
        ϕ
        :
        
          ∧
          
            k
          
        
        (
        V
        )
        →
        X
      
    
    {\displaystyle \phi :\wedge ^{k}(V)\rightarrow X}
   with 
  
    
      
        f
        =
        ϕ
        ∘
        w
        .
      
    
    {\displaystyle f=\phi \circ w.}
    This universal property characterizes the space 
  
    
      
        
          ∧
          
            k
          
        
        (
        V
        )
      
    
    {\displaystyle \wedge ^{k}(V)}
   and can serve as its definition.

Alternating multilinear forms
The above discussion specializes to the case when X = K, the base field.  In this case an alternating multilinear function

  
    
      
        f
        :
        
          V
          
            k
          
        
        →
        K
         
      
    
    {\displaystyle f:V^{k}\to K\ }
  is called an alternating multilinear form.  The set of all alternating multilinear forms is a vector space, as the sum of two such maps, or the product of such a map with a scalar, is again alternating.  By the universal property of the exterior power, the space of alternating forms of degree k on V is naturally isomorphic with the dual vector space 
  
    
      
        
          
            (
          
        
        
          ⋀
          
            k
          
        
        (
        V
        )
        
          
            
              )
            
          
          
            ∗
          
        
        .
      
    
    {\textstyle {\bigl (}\bigwedge \nolimits ^{k}(V){\bigr )}^{*}.}
    If V is finite-dimensional, then the latter is naturally isomorphic to 
  
    
      
        
          ⋀
          
            k
          
        
        
          (
          
            V
            
              ∗
            
          
          )
        
        .
      
    
    {\textstyle \bigwedge \nolimits ^{k}\left(V^{*}\right).}
    In particular, if V is n-dimensional, the dimension of the space of alternating maps from Vk to K is the binomial coefficient 
  
    
      
        
          
            
              (
            
            
              n
              k
            
            
              )
            
          
        
        .
      
    
    {\textstyle {\binom {n}{k}}.}
  
Under such identification, the exterior product takes a concrete form: it produces a new anti-symmetric map from two given ones.  Suppose ω : Vk → K and η : Vm → K are two anti-symmetric maps.  As in the case of tensor products of multilinear maps, the number of variables of their exterior product is the sum of the numbers of their variables.  Depending on the choice of identification of elements of exterior power with multilinear forms, the exterior product is defined as

  
    
      
        ω
        ∧
        η
        =
        Alt
        ⁡
        (
        ω
        ⊗
        η
        )
      
    
    {\displaystyle \omega \wedge \eta =\operatorname {Alt} (\omega \otimes \eta )}
  or as

  
    
      
        ω
        
          
            
              ∧
              ˙
            
          
        
        η
        =
        
          
            
              (
              k
              +
              m
              )
              !
            
            
              k
              !
              
              m
              !
            
          
        
        Alt
        ⁡
        (
        ω
        ⊗
        η
        )
        ,
      
    
    {\displaystyle \omega {\dot {\wedge }}\eta ={\frac {(k+m)!}{k!\,m!}}\operatorname {Alt} (\omega \otimes \eta ),}
  where, if the characteristic of the base field K is 0, the alternation Alt of a multilinear map is defined to be the average of the sign-adjusted values over all the permutations of its variables:

  
    
      
        Alt
        ⁡
        (
        ω
        )
        (
        
          x
          
            1
          
        
        ,
        …
        ,
        
          x
          
            k
          
        
        )
        =
        
          
            1
            
              k
              !
            
          
        
        
          ∑
          
            σ
            ∈
            
              S
              
                k
              
            
          
        
        sgn
        ⁡
        (
        σ
        )
        
        ω
        (
        
          x
          
            σ
            (
            1
            )
          
        
        ,
        …
        ,
        
          x
          
            σ
            (
            k
            )
          
        
        )
        .
      
    
    {\displaystyle \operatorname {Alt} (\omega )(x_{1},\ldots ,x_{k})={\frac {1}{k!}}\sum _{\sigma \in S_{k}}\operatorname {sgn} (\sigma )\,\omega (x_{\sigma (1)},\ldots ,x_{\sigma (k)}).}
  When the field K has finite characteristic, an equivalent version of the second expression without any factorials or any constants is well-defined:

  
    
      
        
          ω
          
            
              
                ∧
                ˙
              
            
          
          η
          (
          
            x
            
              1
            
          
          ,
          …
          ,
          
            x
            
              k
              +
              m
            
          
          )
        
        =
        
          ∑
          
            σ
            ∈
            
              
                S
                h
              
              
                k
                ,
                m
              
            
          
        
        sgn
        ⁡
        (
        σ
        )
        
        ω
        (
        
          x
          
            σ
            (
            1
            )
          
        
        ,
        …
        ,
        
          x
          
            σ
            (
            k
            )
          
        
        )
        
        η
        (
        
          x
          
            σ
            (
            k
            +
            1
            )
          
        
        ,
        …
        ,
        
          x
          
            σ
            (
            k
            +
            m
            )
          
        
        )
        ,
      
    
    {\displaystyle {\omega {\dot {\wedge }}\eta (x_{1},\ldots ,x_{k+m})}=\sum _{\sigma \in \mathrm {Sh} _{k,m}}\operatorname {sgn} (\sigma )\,\omega (x_{\sigma (1)},\ldots ,x_{\sigma (k)})\,\eta (x_{\sigma (k+1)},\ldots ,x_{\sigma (k+m)}),}
  where here Shk,m ⊂ Sk+m is the subset of (k,m) shuffles: permutations σ of the set {1, 2, ..., k + m} such that σ(1) < σ(2) < ⋯ < σ(k), and σ(k + 1) < σ(k + 2) < ⋯ < σ(k + m). As this might look very specific and fine tuned, an equivalent raw version is to sum in the above formula over permutations in  left cosets of Sk+m/Sk x Sm.

Interior product
Suppose that V is finite-dimensional.  If V∗ denotes the dual space to the vector space V, then for each α ∈ V∗, it is possible to define an antiderivation on the algebra 
  
    
      
        ⋀
        (
        V
        )
        ,
      
    
    {\textstyle \bigwedge (V),}
  

  
    
      
        
          i
          
            α
          
        
        :
        
          
            
              ⋀
            
          
          
            k
          
        
        V
        →
        
          
            
              ⋀
            
          
          
            k
            −
            1
          
        
        V
        .
      
    
    {\displaystyle i_{\alpha }:{\textstyle \bigwedge }^{k}V\rightarrow {\textstyle \bigwedge }^{k-1}V.}
  This derivation is called the interior product with α, or sometimes the insertion operator, or contraction by α.
Suppose that 
  
    
      
        w
        ∈
        
          ⋀
          
            k
          
        
        V
        .
      
    
    {\textstyle w\in \bigwedge \nolimits ^{k}V.}
    Then w is a multilinear mapping of V∗ to K, so it is defined by its values on the k-fold Cartesian product V∗ × V∗ × ... × V∗.  If u1, u2, ..., uk−1 are k − 1 elements of V∗, then define

  
    
      
        (
        
          i
          
            α
          
        
        
          
            w
          
        
        )
        (
        
          u
          
            1
          
        
        ,
        
          u
          
            2
          
        
        ,
        …
        ,
        
          u
          
            k
            −
            1
          
        
        )
        =
        
          
            w
          
        
        (
        α
        ,
        
          u
          
            1
          
        
        ,
        
          u
          
            2
          
        
        ,
        …
        ,
        
          u
          
            k
            −
            1
          
        
        )
        .
      
    
    {\displaystyle (i_{\alpha }{\mathbf {w} })(u_{1},u_{2},\ldots ,u_{k-1})={\mathbf {w} }(\alpha ,u_{1},u_{2},\ldots ,u_{k-1}).}
  Additionally, let iαf = 0 whenever f is a pure scalar (i.e., belonging to 
  
    
      
        
          ⋀
          
            0
          
        
        V
      
    
    {\textstyle \bigwedge \nolimits ^{0}V}
  ).

Axiomatic characterization and properties
The interior product satisfies the following properties:

For each k and each α ∈ V∗,  (By convention, 
  
    
      
        
          ⋀
          
            −
            1
          
        
        V
        =
        {
        0
        }
        .
      
    
    {\textstyle \bigwedge \nolimits ^{-1}V=\{0\}.}
  )
If v is an element of V (
  
    
      
        =
        
          ⋀
          
            1
          
        
        V
      
    
    {\textstyle =\bigwedge \nolimits ^{1}V}
  ), then iαv = α(v) is the dual pairing between elements of V and elements of V∗.
For each α ∈ V∗, iα is a graded derivation of degree −1:These three properties are sufficient to characterize the interior product as well as define it in the general infinite-dimensional case.
Further properties of the interior product include:

  
    
      
        
          i
          
            α
          
        
        ∘
        
          i
          
            α
          
        
        =
        0.
      
    
    {\displaystyle i_{\alpha }\circ i_{\alpha }=0.}
  

  
    
      
        
          i
          
            α
          
        
        ∘
        
          i
          
            β
          
        
        =
        −
        
          i
          
            β
          
        
        ∘
        
          i
          
            α
          
        
        .
      
    
    {\displaystyle i_{\alpha }\circ i_{\beta }=-i_{\beta }\circ i_{\alpha }.}

Hodge duality
Suppose that V has finite dimension n.  Then the interior product induces a canonical isomorphism of vector spaces

  
    
      
        
          
            
              ⋀
            
          
          
            k
          
        
        (
        
          V
          
            ∗
          
        
        )
        ⊗
        
          
            
              ⋀
            
          
          
            n
          
        
        (
        V
        )
        →
        
          
            
              ⋀
            
          
          
            n
            −
            k
          
        
        (
        V
        )
      
    
    {\displaystyle {\textstyle \bigwedge }^{k}(V^{*})\otimes {\textstyle \bigwedge }^{n}(V)\to {\textstyle \bigwedge }^{n-k}(V)}
  by the recursive definition

  
    
      
        
          i
          
            α
            ∧
            β
          
        
        =
        
          i
          
            β
          
        
        ∘
        
          i
          
            α
          
        
        .
      
    
    {\displaystyle i_{\alpha \wedge \beta }=i_{\beta }\circ i_{\alpha }.}
  In the geometrical setting, a non-zero element of the top exterior power 
  
    
      
        
          ⋀
          
            n
          
        
        (
        V
        )
      
    
    {\textstyle \bigwedge \nolimits ^{n}(V)}
   (which is a one-dimensional vector space) is sometimes called a volume form (or orientation form, although this term may sometimes lead to ambiguity).  The name orientation form comes from the fact that a choice of preferred top element determines an orientation of the whole exterior algebra, since it is tantamount to fixing an ordered basis of the vector space.  Relative to the preferred volume form σ, the isomorphism is given explicitly by

  
    
      
        
          
            
              ⋀
            
          
          
            k
          
        
        (
        
          V
          
            ∗
          
        
        )
        →
        
          
            
              ⋀
            
          
          
            n
            −
            k
          
        
        (
        V
        )
        :
        α
        ↦
        
          i
          
            α
          
        
        σ
        .
      
    
    {\displaystyle {\textstyle \bigwedge }^{k}(V^{*})\to {\textstyle \bigwedge }^{n-k}(V):\alpha \mapsto i_{\alpha }\sigma .}
  If, in addition to a volume form, the vector space V is equipped with an inner product identifying V with V∗, then the resulting isomorphism is called the Hodge star operator, which maps an element to its Hodge dual:

  
    
      
        ⋆
        :
        
          
            
              ⋀
            
          
          
            k
          
        
        (
        V
        )
        →
        
          
            
              ⋀
            
          
          
            n
            −
            k
          
        
        (
        V
        )
        .
      
    
    {\displaystyle \star :{\textstyle \bigwedge }^{k}(V)\rightarrow {\textstyle \bigwedge }^{n-k}(V).}
  The composition of 
  
    
      
        ⋆
      
    
    {\displaystyle \star }
   with itself maps 
  
    
      
        
          ⋀
          
            k
          
        
        (
        V
        )
      
    
    {\textstyle \bigwedge \nolimits ^{k}(V)}
   → 
  
    
      
        
          ⋀
          
            k
          
        
        (
        V
        )
      
    
    {\textstyle \bigwedge \nolimits ^{k}(V)}
   and is always a scalar multiple of the identity map.  In most applications, the volume form is compatible with the inner product in the sense that it is an exterior product of an orthonormal basis of V.  In this case,

  
    
      
        ⋆
        ∘
        ⋆
        :
        
          
            
              ⋀
            
          
          
            k
          
        
        (
        V
        )
        →
        
          
            
              ⋀
            
          
          
            k
          
        
        (
        V
        )
        =
        (
        −
        1
        
          )
          
            k
            (
            n
            −
            k
            )
            +
            q
          
        
        
          i
          d
        
      
    
    {\displaystyle \star \circ \star :{\textstyle \bigwedge }^{k}(V)\to {\textstyle \bigwedge }^{k}(V)=(-1)^{k(n-k)+q}\mathrm {id} }
  where id is the identity mapping, and the inner product has metric signature (p, q) — p pluses and q minuses.

Inner product
For V a finite-dimensional space, an inner product (or a pseudo-Euclidean inner product) on V defines an isomorphism of V with V∗, and so also an isomorphism of 
  
    
      
        
          ⋀
          
            k
          
        
        V
      
    
    {\textstyle \bigwedge \nolimits ^{k}V}
   with 
  
    
      
        
          
            (
          
        
        
          ⋀
          
            k
          
        
        V
        
          
            
              )
            
          
          
            ∗
          
        
        .
      
    
    {\textstyle {\bigl (}\bigwedge \nolimits ^{k}V{\bigr )}^{*}.}
    The pairing between these two spaces also takes the form of an inner product.  On decomposable k-vectors,

  
    
      
        
          ⟨
          
            
              v
              
                1
              
            
            ∧
            ⋯
            ∧
            
              v
              
                k
              
            
            ,
            
              w
              
                1
              
            
            ∧
            ⋯
            ∧
            
              w
              
                k
              
            
          
          ⟩
        
        =
        det
        
          
            (
          
        
        ⟨
        
          v
          
            i
          
        
        ,
        
          w
          
            j
          
        
        ⟩
        
          
            )
          
        
        ,
      
    
    {\displaystyle \left\langle v_{1}\wedge \cdots \wedge v_{k},w_{1}\wedge \cdots \wedge w_{k}\right\rangle =\det {\bigl (}\langle v_{i},w_{j}\rangle {\bigr )},}
  the determinant of the matrix of inner products.  In the special case vi = wi, the inner product is the square norm of the k-vector, given by the determinant of the Gramian matrix (⟨vi, vj⟩).  This is then extended bilinearly (or sesquilinearly in the complex case) to a non-degenerate inner product on 
  
    
      
        
          ⋀
          
            k
          
        
        V
        .
      
    
    {\textstyle \bigwedge \nolimits ^{k}V.}
    If ei, i = 1, 2, ..., n, form an orthonormal basis of V, then the vectors of the form

  
    
      
        
          e
          
            
              i
              
                1
              
            
          
        
        ∧
        ⋯
        ∧
        
          e
          
            
              i
              
                k
              
            
          
        
        ,
        
        
          i
          
            1
          
        
        <
        ⋯
        <
        
          i
          
            k
          
        
        ,
      
    
    {\displaystyle e_{i_{1}}\wedge \cdots \wedge e_{i_{k}},\quad i_{1}<\cdots <i_{k},}
  constitute an orthonormal basis for 
  
    
      
        
          ⋀
          
            k
          
        
        (
        V
        )
      
    
    {\textstyle \bigwedge \nolimits ^{k}(V)}
  , a statement equivalent to the Cauchy–Binet formula.
With respect to the inner product, exterior multiplication and the interior product are mutually adjoint.  Specifically, for 
  
    
      
        v
        ∈
        
          ⋀
          
            k
            −
            1
          
        
        (
        V
        )
        ,
      
    
    {\textstyle v\in \bigwedge \nolimits ^{k-1}(V),}
   
  
    
      
        w
        ∈
        
          ⋀
          
            k
          
        
        (
        V
        )
        ,
      
    
    {\textstyle w\in \bigwedge \nolimits ^{k}(V),}
   and 
  
    
      
        x
        ∈
        V
        ,
      
    
    {\textstyle x\in V,}
  

  
    
      
        ⟨
        x
        ∧
        
          v
        
        ,
        
          w
        
        ⟩
        =
        ⟨
        
          v
        
        ,
        
          i
          
            
              x
              
                ♭
              
            
          
        
        
          w
        
        ⟩
      
    
    {\displaystyle \langle x\wedge \mathbf {v} ,\mathbf {w} \rangle =\langle \mathbf {v} ,i_{x^{\flat }}\mathbf {w} \rangle }
  where x♭ ∈ V∗ is the musical isomorphism, the linear functional defined by

  
    
      
        
          x
          
            ♭
          
        
        (
        y
        )
        =
        ⟨
        x
        ,
        y
        ⟩
      
    
    {\displaystyle x^{\flat }(y)=\langle x,y\rangle }
  for all y ∈ V.  This property completely characterizes the inner product on the exterior algebra.
Indeed, more generally for 
  
    
      
        v
        ∈
        
          ⋀
          
            k
            −
            l
          
        
        (
        V
        )
        ,
      
    
    {\textstyle v\in \bigwedge \nolimits ^{k-l}(V),}
   
  
    
      
        w
        ∈
        
          ⋀
          
            k
          
        
        (
        V
        )
        ,
      
    
    {\textstyle w\in \bigwedge \nolimits ^{k}(V),}
   and 
  
    
      
        x
        ∈
        
          ⋀
          
            l
          
        
        (
        V
        )
        ,
      
    
    {\textstyle x\in \bigwedge \nolimits ^{l}(V),}
   iteration of the above adjoint properties gives

  
    
      
        ⟨
        
          x
        
        ∧
        
          v
        
        ,
        
          w
        
        ⟩
        =
        ⟨
        
          v
        
        ,
        
          i
          
            
              
                x
              
              
                ♭
              
            
          
        
        
          w
        
        ⟩
      
    
    {\displaystyle \langle \mathbf {x} \wedge \mathbf {v} ,\mathbf {w} \rangle =\langle \mathbf {v} ,i_{\mathbf {x} ^{\flat }}\mathbf {w} \rangle }
  where now 
  
    
      
        
          x
          
            ♭
          
        
        ∈
        
          ⋀
          
            l
          
        
        
          (
          
            V
            
              ∗
            
          
          )
        
        ≃
        
          
            (
          
        
        
          ⋀
          
            l
          
        
        (
        V
        )
        
          
            
              )
            
          
          
            ∗
          
        
      
    
    {\textstyle x^{\flat }\in \bigwedge \nolimits ^{l}\left(V^{*}\right)\simeq {\bigl (}\bigwedge \nolimits ^{l}(V){\bigr )}^{*}}
   is the dual l-vector defined by

  
    
      
        
          
            x
          
          
            ♭
          
        
        (
        
          y
        
        )
        =
        ⟨
        
          x
        
        ,
        
          y
        
        ⟩
      
    
    {\displaystyle \mathbf {x} ^{\flat }(\mathbf {y} )=\langle \mathbf {x} ,\mathbf {y} \rangle }
  for all 
  
    
      
        y
        ∈
        
          ⋀
          
            l
          
        
        (
        V
        )
        .
      
    
    {\textstyle y\in \bigwedge \nolimits ^{l}(V).}

Bialgebra structure
There is a correspondence between the graded dual of the graded algebra 
  
    
      
        ⋀
        (
        V
        )
      
    
    {\textstyle \bigwedge (V)}
   and alternating multilinear forms on V.  The exterior algebra (as well as the symmetric algebra) inherits a bialgebra structure, and, indeed, a Hopf algebra structure, from the tensor algebra.  See the article on tensor algebras for a detailed treatment of the topic.
The exterior product of multilinear forms defined above is dual to a coproduct defined on 
  
    
      
        ⋀
        (
        V
        )
        ,
      
    
    {\textstyle \bigwedge (V),}
   giving the structure of a coalgebra.  The coproduct is a linear function Δ : 
  
    
      
        ⋀
        (
        V
        )
      
    
    {\textstyle \bigwedge (V)}
   → 
  
    
      
        ⋀
        (
        V
        )
      
    
    {\textstyle \bigwedge (V)}
   ⊗ 
  
    
      
        ⋀
        (
        V
        )
      
    
    {\textstyle \bigwedge (V)}
   which is given by

  
    
      
        Δ
        (
        v
        )
        =
        1
        ⊗
        v
        +
        v
        ⊗
        1
      
    
    {\displaystyle \Delta (v)=1\otimes v+v\otimes 1}
  on elements v∈V.  The symbol 1 stands for the unit element of the field K.  Recall that K ⊂ 
  
    
      
        ⋀
        (
        V
        )
      
    
    {\textstyle \bigwedge (V)}
  , so that the above really does lie in 
  
    
      
        ⋀
        (
        V
        )
      
    
    {\textstyle \bigwedge (V)}
   ⊗ 
  
    
      
        ⋀
        (
        V
        )
      
    
    {\textstyle \bigwedge (V)}
  . This definition of the coproduct is lifted to the full space 
  
    
      
        ⋀
        (
        V
        )
      
    
    {\textstyle \bigwedge (V)}
   by (linear) homomorphism. 
The correct form of this homomorphism is not what one might naively write, but has to be the one carefully defined in the coalgebra article.  In this case, one obtains

  
    
      
        Δ
        (
        v
        ∧
        w
        )
        =
        1
        ⊗
        (
        v
        ∧
        w
        )
        +
        v
        ⊗
        w
        −
        w
        ⊗
        v
        +
        (
        v
        ∧
        w
        )
        ⊗
        1.
      
    
    {\displaystyle \Delta (v\wedge w)=1\otimes (v\wedge w)+v\otimes w-w\otimes v+(v\wedge w)\otimes 1.}
  Expanding this out in detail, one obtains the following expression on decomposable elements:

  
    
      
        Δ
        (
        
          x
          
            1
          
        
        ∧
        ⋯
        ∧
        
          x
          
            k
          
        
        )
        =
        
          ∑
          
            p
            =
            0
          
          
            k
          
        
        
        
          ∑
          
            σ
            ∈
            S
            h
            (
            p
            +
            1
            ,
            k
            −
            p
            )
          
        
        
        sgn
        ⁡
        (
        σ
        )
        (
        
          x
          
            σ
            (
            0
            )
          
        
        ∧
        ⋯
        ∧
        
          x
          
            σ
            (
            p
            )
          
        
        )
        ⊗
        (
        
          x
          
            σ
            (
            p
            +
            1
            )
          
        
        ∧
        ⋯
        ∧
        
          x
          
            σ
            (
            k
            )
          
        
        )
        .
      
    
    {\displaystyle \Delta (x_{1}\wedge \cdots \wedge x_{k})=\sum _{p=0}^{k}\;\sum _{\sigma \in Sh(p+1,k-p)}\;\operatorname {sgn} (\sigma )(x_{\sigma (0)}\wedge \cdots \wedge x_{\sigma (p)})\otimes (x_{\sigma (p+1)}\wedge \cdots \wedge x_{\sigma (k)}).}
  where the second summation is taken over all (p+1, k−p)-shuffles.  The above is written with a notational trick, to keep track of the field element 1: the trick is to write 
  
    
      
        
          x
          
            0
          
        
        =
        1
        ,
      
    
    {\displaystyle x_{0}=1,}
   and this is shuffled into various locations during the expansion of the sum over shuffles.  The shuffle follows directly from the first axiom of a co-algebra: the relative order of the elements 
  
    
      
        
          x
          
            k
          
        
      
    
    {\displaystyle x_{k}}
   is preserved in the riffle shuffle: the riffle shuffle merely splits the ordered sequence into two ordered sequences, one on the left, and one on the right.
Observe that the coproduct preserves the grading of the algebra.  Extending to the full space 
  
    
      
        ⋀
        (
        V
        )
        ,
      
    
    {\textstyle \bigwedge (V),}
   one has

  
    
      
        Δ
        :
        
          
            
              ⋀
            
          
          
            k
          
        
        (
        V
        )
        →
        
          ⨁
          
            p
            =
            0
          
          
            k
          
        
        
          
            
              ⋀
            
          
          
            p
          
        
        (
        V
        )
        ⊗
        
          
            
              ⋀
            
          
          
            k
            −
            p
          
        
        (
        V
        )
      
    
    {\displaystyle \Delta :{\textstyle \bigwedge }^{k}(V)\to \bigoplus _{p=0}^{k}{\textstyle \bigwedge }^{p}(V)\otimes {\textstyle \bigwedge }^{k-p}(V)}
  The tensor symbol ⊗ used in this section should be understood with some caution: it is not the same tensor symbol as the one being used in the definition of the alternating product.  Intuitively, it is perhaps easiest to think it as just another, but different, tensor product: it is still (bi-)linear, as tensor products should be, but it is the product that is appropriate for the definition of a bialgebra, that is, for creating the object 
  
    
      
        ⋀
        (
        V
        )
      
    
    {\textstyle \bigwedge (V)}
   ⊗ 
  
    
      
        ⋀
        (
        V
        )
      
    
    {\textstyle \bigwedge (V)}
  . Any lingering doubt can be shaken by pondering the equalities (1 ⊗ v) ∧ (1 ⊗ w) = 1 ⊗ (v ∧ w) and (v ⊗ 1) ∧ (1 ⊗ w) = v ⊗ w, which follow from the definition of the coalgebra, as opposed to naive manipulations involving the tensor and wedge symbols.  This distinction is developed in greater detail in the article on tensor algebras.  Here, there is much less of a problem, in that the alternating product ∧ clearly corresponds to multiplication in the bialgebra, leaving the symbol ⊗ free for use in the definition of the bialgebra.  In practice, this presents no particular problem, as long as one avoids the fatal trap of replacing alternating sums of ⊗ by the wedge symbol, with one exception.  One can construct an alternating product from ⊗, with the understanding that it works in a different space.  Immediately below, an example is given: the alternating product for the dual space can be given in terms of the coproduct.  The construction of the bialgebra here parallels the construction in the tensor algebra article almost exactly, except for the need to correctly track the alternating signs for the exterior algebra.
In terms of the coproduct, the exterior product on the dual space is just the graded dual of the coproduct:

  
    
      
        (
        α
        ∧
        β
        )
        (
        
          x
          
            1
          
        
        ∧
        ⋯
        ∧
        
          x
          
            k
          
        
        )
        =
        (
        α
        ⊗
        β
        )
        
          (
          
            Δ
            (
            
              x
              
                1
              
            
            ∧
            ⋯
            ∧
            
              x
              
                k
              
            
            )
          
          )
        
      
    
    {\displaystyle (\alpha \wedge \beta )(x_{1}\wedge \cdots \wedge x_{k})=(\alpha \otimes \beta )\left(\Delta (x_{1}\wedge \cdots \wedge x_{k})\right)}
  where the tensor product on the right-hand side is of multilinear linear maps (extended by zero on elements of incompatible homogeneous degree: more precisely, α ∧ β = ε ∘ (α ⊗ β) ∘ Δ, where ε is the counit, as defined presently).
The counit is the homomorphism ε : 
  
    
      
        ⋀
        (
        V
        )
      
    
    {\textstyle \bigwedge (V)}
   → K that returns the 0-graded component of its argument.  The coproduct and counit, along with the exterior product, define the structure of a bialgebra on the exterior algebra.
With an antipode defined on homogeneous elements by 
  
    
      
        S
        (
        x
        )
        =
        (
        −
        1
        
          )
          
            
              
                (
              
              
                
                  
                    deg
                  
                  
                  x
                  
                  +
                  1
                
                2
              
              
                )
              
            
          
        
        x
        ,
      
    
    {\displaystyle S(x)=(-1)^{\binom {{\text{deg}}\,x\,+1}{2}}x,}
   the exterior algebra is furthermore a Hopf algebra.

Functoriality
Suppose that V and W are a pair of vector spaces and f : V → W is a linear map.  Then, by the universal property, there exists a unique homomorphism of graded algebras

  
    
      
        
          
            ⋀
          
        
        (
        f
        )
        :
        
          
            ⋀
          
        
        (
        V
        )
        →
        
          
            ⋀
          
        
        (
        W
        )
      
    
    {\displaystyle {\textstyle \bigwedge }(f):{\textstyle \bigwedge }(V)\rightarrow {\textstyle \bigwedge }(W)}
  such that

  
    
      
        
          
            ⋀
          
        
        (
        f
        )
        
          |
          
            
            
              
                
                  
                    ⋀
                  
                
                
                  1
                
              
              (
              V
              )
            
          
          
        
        =
        f
        :
        V
        =
        
          
            
              ⋀
            
          
          
            1
          
        
        (
        V
        )
        →
        W
        =
        
          
            
              ⋀
            
          
          
            1
          
        
        (
        W
        )
        .
      
    
    {\displaystyle {\textstyle \bigwedge }(f)\left|_{{\textstyle \bigwedge }^{1}(V)}\right.=f:V={\textstyle \bigwedge }^{1}(V)\rightarrow W={\textstyle \bigwedge }^{1}(W).}
  In particular, 
  
    
      
        ⋀
        
          (
          f
          )
        
      
    
    {\textstyle \bigwedge \left(f\right)}
   preserves homogeneous degree.  The k-graded components of 
  
    
      
        ⋀
        
          (
          f
          )
        
      
    
    {\textstyle \bigwedge \left(f\right)}
   are given on decomposable elements by

  
    
      
        
          
            ⋀
          
        
        (
        f
        )
        (
        
          x
          
            1
          
        
        ∧
        ⋯
        ∧
        
          x
          
            k
          
        
        )
        =
        f
        (
        
          x
          
            1
          
        
        )
        ∧
        ⋯
        ∧
        f
        (
        
          x
          
            k
          
        
        )
        .
      
    
    {\displaystyle {\textstyle \bigwedge }(f)(x_{1}\wedge \cdots \wedge x_{k})=f(x_{1})\wedge \cdots \wedge f(x_{k}).}
  Let

  
    
      
        
          
            
              ⋀
            
          
          
            k
          
        
        (
        f
        )
        =
        
          
            ⋀
          
        
        (
        f
        )
        
          |
          
            
            
              
                
                  
                    ⋀
                  
                
                
                  k
                
              
              (
              V
              )
            
          
          
        
        :
        
          
            
              ⋀
            
          
          
            k
          
        
        (
        V
        )
        →
        
          
            
              ⋀
            
          
          
            k
          
        
        (
        W
        )
        .
      
    
    {\displaystyle {\textstyle \bigwedge }^{k}(f)={\textstyle \bigwedge }(f)\left|_{{\textstyle \bigwedge }^{k}(V)}\right.:{\textstyle \bigwedge }^{k}(V)\rightarrow {\textstyle \bigwedge }^{k}(W).}
  The components of the transformation 
  
    
      
        
          ⋀
          
            k
          
        
        
          (
          f
          )
        
      
    
    {\textstyle \bigwedge \nolimits ^{k}\left(f\right)}
   relative to a basis of V and W is the matrix of k × k minors of f.  In particular, if V = W and V is of finite dimension n, then 
  
    
      
        
          ⋀
          
            n
          
        
        
          (
          f
          )
        
      
    
    {\textstyle \bigwedge \nolimits ^{n}\left(f\right)}
   is a mapping of a one-dimensional vector space 
  
    
      
        
          ⋀
          
            n
          
        
        (
        V
        )
      
    
    {\textstyle \bigwedge \nolimits ^{n}(V)}
   to itself, and is therefore given by a scalar: the determinant of f.

Exactness
If 
  
    
      
        0
        →
        U
        →
        V
        →
        W
        →
        0
      
    
    {\displaystyle 0\to U\to V\to W\to 0}
   is a short exact sequence of vector spaces, then

  
    
      
        0
        →
        
          
            
              ⋀
            
          
          
            1
          
        
        (
        U
        )
        ∧
        
          
            ⋀
          
        
        (
        V
        )
        →
        
          
            ⋀
          
        
        (
        V
        )
        →
        
          
            ⋀
          
        
        (
        W
        )
        →
        0
      
    
    {\displaystyle 0\to {\textstyle \bigwedge }^{1}(U)\wedge {\textstyle \bigwedge }(V)\to {\textstyle \bigwedge }(V)\to {\textstyle \bigwedge }(W)\to 0}
  is an exact sequence of graded vector spaces, as is

  
    
      
        0
        →
        ⋀
        (
        U
        )
        →
        ⋀
        (
        V
        )
        .
      
    
    {\textstyle 0\to \bigwedge (U)\to \bigwedge (V).}

Direct sums
In particular, the exterior algebra of a direct sum is isomorphic to the tensor product of the exterior algebras:

  
    
      
        
          
            ⋀
          
        
        (
        V
        ⊕
        W
        )
        ≅
        
          
            ⋀
          
        
        (
        V
        )
        ⊗
        
          
            ⋀
          
        
        (
        W
        )
        .
      
    
    {\displaystyle {\textstyle \bigwedge }(V\oplus W)\cong {\textstyle \bigwedge }(V)\otimes {\textstyle \bigwedge }(W).}
  This is a graded isomorphism; i.e.,

  
    
      
        
          
            
              ⋀
            
          
          
            k
          
        
        (
        V
        ⊕
        W
        )
        ≅
        
          ⨁
          
            p
            +
            q
            =
            k
          
        
        
          
            
              ⋀
            
          
          
            p
          
        
        (
        V
        )
        ⊗
        
          
            
              ⋀
            
          
          
            q
          
        
        (
        W
        )
        .
      
    
    {\displaystyle {\textstyle \bigwedge }^{k}(V\oplus W)\cong \bigoplus _{p+q=k}{\textstyle \bigwedge }^{p}(V)\otimes {\textstyle \bigwedge }^{q}(W).}
  In greater generality, for a short exact sequence of vector spaces 
  
    
      
        0
        →
        U
        
          
            →
            f
          
        
        V
        
          
            →
            g
          
        
        W
        →
        0
        ,
      
    
    {\textstyle 0\to U\mathrel {\overset {f}{\to }} V\mathrel {\overset {g}{\to }} W\to 0,}
   there is a natural filtration 

  
    
      
        0
        =
        
          F
          
            0
          
        
        ⊆
        
          F
          
            1
          
        
        ⊆
        ⋯
        ⊆
        
          F
          
            k
          
        
        ⊆
        
          F
          
            k
            +
            1
          
        
        =
        
          
            
              ⋀
            
          
          
            k
          
        
        (
        V
        )
      
    
    {\displaystyle 0=F^{0}\subseteq F^{1}\subseteq \cdots \subseteq F^{k}\subseteq F^{k+1}={\textstyle \bigwedge }^{k}(V)}
  where 
  
    
      
        
          F
          
            p
          
        
      
    
    {\displaystyle F^{p}}
   for 
  
    
      
        p
        ≥
        1
      
    
    {\displaystyle p\geq 1}
   is spanned by elements of the form 
  
    
      
        
          u
          
            1
          
        
        ∧
        …
        ∧
        
          u
          
            k
            +
            1
            −
            p
          
        
        ∧
        
          v
          
            1
          
        
        ∧
        …
        
          v
          
            p
            −
            1
          
        
      
    
    {\displaystyle u_{1}\wedge \ldots \wedge u_{k+1-p}\wedge v_{1}\wedge \ldots v_{p-1}}
   for 
  
    
      
        
          u
          
            i
          
        
        ∈
        U
      
    
    {\displaystyle u_{i}\in U}
   and 
  
    
      
        
          v
          
            i
          
        
        ∈
        V
        .
      
    
    {\displaystyle v_{i}\in V.}
  
The corresponding quotients admit a natural isomorphism

  
    
      
        
          F
          
            p
            +
            1
          
        
        
          /
        
        
          F
          
            p
          
        
        ≅
        
          
            
              ⋀
            
          
          
            k
            −
            p
          
        
        (
        U
        )
        ⊗
        
          
            
              ⋀
            
          
          
            p
          
        
        (
        W
        )
      
    
    {\displaystyle F^{p+1}/F^{p}\cong {\textstyle \bigwedge }^{k-p}(U)\otimes {\textstyle \bigwedge }^{p}(W)}
   given by 
  
    
      
        
          u
          
            1
          
        
        ∧
        …
        ∧
        
          u
          
            k
            −
            p
          
        
        ∧
        
          v
          
            1
          
        
        ∧
        …
        ∧
        
          v
          
            p
          
        
        ↦
        
          u
          
            1
          
        
        ∧
        …
        ∧
        
          u
          
            k
            −
            p
          
        
        ⊗
        g
        (
        
          v
          
            1
          
        
        )
        ∧
        …
        ∧
        g
        (
        
          v
          
            p
          
        
        )
        .
      
    
    {\displaystyle u_{1}\wedge \ldots \wedge u_{k-p}\wedge v_{1}\wedge \ldots \wedge v_{p}\mapsto u_{1}\wedge \ldots \wedge u_{k-p}\otimes g(v_{1})\wedge \ldots \wedge g(v_{p}).}
  In particular, if U is 1-dimensional then

  
    
      
        0
        →
        U
        ⊗
        
          
            
              ⋀
            
          
          
            k
            −
            1
          
        
        (
        W
        )
        →
        
          
            
              ⋀
            
          
          
            k
          
        
        (
        V
        )
        →
        
          
            
              ⋀
            
          
          
            k
          
        
        (
        W
        )
        →
        0
      
    
    {\displaystyle 0\to U\otimes {\textstyle \bigwedge }^{k-1}(W)\to {\textstyle \bigwedge }^{k}(V)\to {\textstyle \bigwedge }^{k}(W)\to 0}
  is exact, and if W is 1-dimensional then

  
    
      
        0
        →
        
          
            
              ⋀
            
          
          
            k
          
        
        (
        U
        )
        →
        
          
            
              ⋀
            
          
          
            k
          
        
        (
        V
        )
        →
        
          
            
              ⋀
            
          
          
            k
            −
            1
          
        
        (
        U
        )
        ⊗
        W
        →
        0
      
    
    {\displaystyle 0\to {\textstyle \bigwedge }^{k}(U)\to {\textstyle \bigwedge }^{k}(V)\to {\textstyle \bigwedge }^{k-1}(U)\otimes W\to 0}
  is exact.

Applications
Linear algebra
In applications to linear algebra, the exterior product provides an abstract algebraic manner for describing the determinant and the minors of a matrix.  For instance, it is well known that the determinant of a square matrix is equal to the volume of the parallelotope whose sides are the columns of the matrix (with a sign to track orientation).  This suggests that the determinant can be defined in terms of the exterior product of the column vectors.  Likewise, the k × k minors of a matrix can be defined by looking at the exterior products of column vectors chosen k at a time.  These ideas can be extended not just to matrices but to linear transformations as well: the determinant of a linear transformation is the factor by which it scales the oriented volume of any given reference parallelotope.  So the determinant of a linear transformation can be defined in terms of what the transformation does to the top exterior power.  The action of a transformation on the lesser exterior powers gives a basis-independent way to talk about the minors of the transformation.

Technical details: Definitions
Let 
  
    
      
        V
      
    
    {\displaystyle V}
   be an n-dimensional vector space over field 
  
    
      
        K
      
    
    {\displaystyle K}
   with basis 
  
    
      
        {
        
          e
          
            1
          
        
        ,
        …
        ,
        
          e
          
            n
          
        
        }
        .
      
    
    {\displaystyle \{e_{1},\ldots ,e_{n}\}.}
  

For 
  
    
      
        A
        ∈
        End
        ⁡
        (
        V
        )
        ,
      
    
    {\displaystyle A\in \operatorname {End} (V),}
   define 
  
    
      
        
          ⋀
          
            k
          
        
        A
        ∈
        End
        ⁡
        
          
            (
          
        
        
          ⋀
          
            k
          
        
        V
        
          
            )
          
        
      
    
    {\textstyle \bigwedge ^{k}A\in \operatorname {End} {\bigl (}\bigwedge ^{k}V{\bigr )}}
   on simple tensors by  and expand the definition linearly to all tensors.  More generally, we can define 
  
    
      
        
          ⋀
          
            p
          
        
        
          A
          
            k
          
        
        ∈
        End
        ⁡
        
          
            (
          
        
        
          ⋀
          
            p
          
        
        V
        
          
            )
          
        
        ,
        (
        p
        ≥
        k
        )
      
    
    {\textstyle \bigwedge ^{p}A^{k}\in \operatorname {End} {\bigl (}\bigwedge ^{p}V{\bigr )},(p\geq k)}
   on simple tensors by  i.e. choose k components on which A would act, then sum up all results obtained from different choices. For 
  
    
      
        k
        =
        p
        =
        n
      
    
    {\displaystyle k=p=n}
  , this recovers the determinant of 
  
    
      
        A
      
    
    {\displaystyle A}
  . For 
  
    
      
        k
        =
        1
        ,
        p
        =
        n
      
    
    {\displaystyle k=1,p=n}
  , this recovers the trace of 
  
    
      
        A
      
    
    {\displaystyle A}
  . If 
  
    
      
        p
        <
        k
        ,
      
    
    {\displaystyle p<k,}
   define 
  
    
      
        
          ⋀
          
            p
          
        
        
          A
          
            k
          
        
        =
        0.
      
    
    {\textstyle \bigwedge ^{p}A^{k}=0.}
    Since 
  
    
      
        
          ⋀
          
            n
          
        
        V
      
    
    {\textstyle \bigwedge ^{n}V}
   is 1-dimensional with basis 
  
    
      
        
          e
          
            1
          
        
        ∧
        ⋯
        ∧
        
          e
          
            n
          
        
        ,
      
    
    {\displaystyle e_{1}\wedge \cdots \wedge e_{n},}
   we can identify 
  
    
      
        
          ⋀
          
            n
          
        
        
          A
          
            k
          
        
      
    
    {\textstyle \bigwedge ^{n}A^{k}}
   with the unique number 
  
    
      
        κ
        ∈
        K
      
    
    {\displaystyle \kappa \in K}
   satisfying 
For 
  
    
      
        φ
        ∈
        End
        ⁡
        
          
            (
          
        
        
          
            
              ⋀
            
          
          
            p
          
        
        V
        
          
            )
          
        
        ,
      
    
    {\displaystyle \varphi \in \operatorname {End} {\bigl (}{\textstyle \bigwedge }^{p}V{\bigr )},}
   define the exterior transpose 
  
    
      
        
          φ
          
            
              T
            
          
        
        ∈
        End
        ⁡
        
          
            (
          
        
        
          ⋀
          
            n
            −
            p
          
        
        V
        
          
            )
          
        
      
    
    {\textstyle \varphi ^{\mathrm {T} }\in \operatorname {End} {\bigl (}\bigwedge ^{n-p}V{\bigr )}}
   to be the unique operator satisfying 
  
    
      
        (
        
          φ
          
            
              T
            
          
        
        
          ω
          
            n
            −
            p
          
        
        )
        ∧
        
          ω
          
            p
          
        
        =
        
          ω
          
            n
            −
            p
          
        
        ∧
        (
        φ
        
          ω
          
            p
          
        
        )
      
    
    {\textstyle (\varphi ^{\mathrm {T} }\omega _{n-p})\wedge \omega _{p}=\omega _{n-p}\wedge (\varphi \omega _{p})}
   for any 
  
    
      
        
          ω
          
            p
          
        
        ∈
        
          
            
              ⋀
            
          
          
            p
          
        
        V
      
    
    {\textstyle \omega _{p}\in {\textstyle \bigwedge }^{p}V}
   and 
  
    
      
        
          ω
          
            n
            −
            p
          
        
        ∈
        
          
            
              ⋀
            
          
          
            n
            −
            p
          
        
        V
        .
      
    
    {\textstyle \omega _{n-p}\in {\textstyle \bigwedge }^{n-p}V.}
  
For 
  
    
      
        A
        ∈
        End
        ⁡
        (
        V
        )
        ,
      
    
    {\displaystyle A\in \operatorname {End} (V),}
   define 
  
    
      
        det
        A
        =
        
          ⋀
          
            n
          
        
        
          A
          
            n
          
        
        ,
      
    
    {\textstyle \det A=\bigwedge ^{n}A^{n},}
   
  
    
      
        Tr
        ⁡
        (
        A
        )
        =
        
          ⋀
          
            n
          
        
        
          A
          
            1
          
        
        ,
      
    
    {\textstyle \operatorname {Tr} (A)=\bigwedge ^{n}A^{1},}
   
  
    
      
        adj
        ⁡
        A
        =
        
          
            (
          
        
        
          ⋀
          
            n
            −
            1
          
        
        
          A
          
            n
            −
            1
          
        
        
          
            
              )
            
          
          
            
              T
            
          
        
        .
      
    
    {\textstyle \operatorname {adj} A={\bigl (}\bigwedge ^{n-1}A^{n-1}{\bigr )}^{\mathrm {T} }.}
    These are equivalent to the previous definitions.

Basic properties
All results obtained from other definitions of the determinant, trace and adjoint can be obtained from this definition (since these definitions are equivalent).  Here are some basic properties related to these new definitions:

  
    
      
        (
        ⋅
        
          )
          
            
              T
            
          
        
      
    
    {\displaystyle (\cdot )^{\mathrm {T} }}
   is 
  
    
      
        K
      
    
    {\displaystyle K}
  -linear.

  
    
      
        (
        A
        B
        
          )
          
            
              T
            
          
        
        =
        
          B
          
            
              T
            
          
        
        
          A
          
            
              T
            
          
        
        .
      
    
    {\displaystyle (AB)^{\mathrm {T} }=B^{\mathrm {T} }A^{\mathrm {T} }.}
  
We have a canonical isomorphism  However, there is no canonical isomorphism between 
  
    
      
        
          ⋀
          
            k
          
        
        V
      
    
    {\textstyle \bigwedge ^{k}V}
   and 
  
    
      
        
          ⋀
          
            n
            −
            k
          
        
        V
        .
      
    
    {\textstyle \bigwedge ^{n-k}V.}
  

  
    
      
        Tr
        ⁡
        
          
            (
          
        
        
          ⋀
          
            k
          
        
        A
        
          
            )
          
        
        =
        
          ⋀
          
            n
          
        
        
          A
          
            k
          
        
        .
      
    
    {\textstyle \operatorname {Tr} {\bigl (}\bigwedge ^{k}A{\bigr )}=\bigwedge ^{n}A^{k}.}
   The entries of the transposed matrix of 
  
    
      
        
          ⋀
          
            k
          
        
        A
      
    
    {\textstyle \bigwedge ^{k}A}
   are 
  
    
      
        k
        ×
        k
      
    
    {\displaystyle k\times k}
  -minors of 
  
    
      
        A
        .
      
    
    {\displaystyle A.}
  
For all 
  
    
      
        k
        ≤
        n
        −
        1
        ,
        p
        ≤
        k
        ,
        A
        ∈
        End
        ⁡
        (
        V
        )
        ,
      
    
    {\displaystyle k\leq n-1,p\leq k,A\in \operatorname {End} (V),}
    In particular,  and hence 

  
    
      
        
          
            (
          
        
        
          
            
              ⋀
            
          
          
            n
            −
            1
          
        
        
          A
          
            p
          
        
        
          
            
              )
            
          
          
            
              T
            
          
        
        =
        
          ∑
          
            q
            =
            0
          
          
            p
          
        
        
          
            (
          
        
        
          
            
              ⋀
            
          
          
            n
          
        
        
          A
          
            p
            −
            q
          
        
        
          
            )
          
        
        (
        −
        A
        
          )
          
            q
          
        
        =
        
          ∑
          
            q
            =
            0
          
          
            p
          
        
        Tr
        ⁡
        
          
            (
          
        
        
          
            
              ⋀
            
          
          
            p
            −
            q
          
        
        A
        
          
            )
          
        
        (
        −
        A
        
          )
          
            q
          
        
        .
      
    
    {\displaystyle {\bigl (}{\textstyle \bigwedge }^{n-1}A^{p}{\bigr )}^{\mathrm {T} }=\sum _{q=0}^{p}{\bigl (}{\textstyle \bigwedge }^{n}A^{p-q}{\bigr )}(-A)^{q}=\sum _{q=0}^{p}\operatorname {Tr} {\bigl (}{\textstyle \bigwedge }^{p-q}A{\bigr )}(-A)^{q}.}
   In particular, 

  
    
      
        Tr
        ⁡
        
          
            (
          
        
        
          
            
              ⋀
            
          
          
            k
          
        
        adj
        ⁡
        A
        
          
            )
          
        
        =
        
          
            
              ⋀
            
          
          
            n
          
        
        (
        adj
        ⁡
        A
        
          )
          
            k
          
        
        =
        (
        det
        A
        
          )
          
            k
            −
            1
          
        
        
          
            (
          
        
        
          
            
              ⋀
            
          
          
            n
          
        
        
          A
          
            n
            −
            k
          
        
        
          
            )
          
        
        =
        (
        det
        A
        
          )
          
            k
            −
            1
          
        
        Tr
        ⁡
        
          
            (
          
        
        
          
            
              ⋀
            
          
          
            n
            −
            k
          
        
        A
        
          
            )
          
        
        .
      
    
    {\displaystyle \operatorname {Tr} {\bigl (}{\textstyle \bigwedge }^{k}\operatorname {adj} A{\bigr )}={\textstyle \bigwedge }^{n}(\operatorname {adj} A)^{k}=(\det A)^{k-1}{\bigl (}{\textstyle \bigwedge }^{n}A^{n-k}{\bigr )}=(\det A)^{k-1}\operatorname {Tr} {\bigl (}{\textstyle \bigwedge }^{n-k}A{\bigr )}.}
  

  
    
      
        Tr
        
        
          
            (
          
        
        
          
            (
          
        
        
          
            
              ⋀
            
          
          
            n
            −
            1
          
        
        
          A
          
            k
          
        
        
          
            
              )
            
          
          
            
              T
            
          
        
        
          
            )
          
        
        =
        (
        n
        −
        k
        )
        
          
            
              ⋀
            
          
          
            n
          
        
        
          A
          
            p
          
        
        =
        (
        n
        −
        k
        )
        Tr
        ⁡
        
          (
          
            
              
                
                  ⋀
                
              
              
                p
              
            
            A
          
          )
        
        .
      
    
    {\displaystyle \operatorname {Tr} \!{\Bigl (}{\bigl (}{\textstyle \bigwedge }^{n-1}A^{k}{\bigr )}^{\mathrm {T} }{\Bigr )}=(n-k){\textstyle \bigwedge }^{n}A^{p}=(n-k)\operatorname {Tr} \left({\textstyle \bigwedge }^{p}A\right).}
  
The characteristic polynomial 
  
    
      
        
          ch
          
            A
          
        
        ⁡
        (
        t
        )
      
    
    {\displaystyle \operatorname {ch} _{A}(t)}
   of 
  
    
      
        A
        ∈
        End
        ⁡
        (
        V
        )
      
    
    {\displaystyle A\in \operatorname {End} (V)}
   can be given by  Similarly,

Leverrier's algorithm
⋀
          
            n
          
        
        
          A
          
            k
          
        
      
    
    {\textstyle \bigwedge ^{n}A^{k}}
   are the coefficients of the 
  
    
      
        (
        −
        t
        
          )
          
            n
            −
            k
          
        
      
    
    {\displaystyle (-t)^{n-k}}
   terms in the characteristic polynomial.  They also appear in the expressions of 
  
    
      
        
          
            (
          
        
        
          ⋀
          
            n
            −
            1
          
        
        
          A
          
            p
          
        
        
          
            
              )
            
          
          
            
              T
            
          
        
      
    
    {\textstyle {\bigl (}\bigwedge ^{n-1}A^{p}{\bigr )}^{\mathrm {T} }}
   and 
  
    
      
        
          ⋀
          
            n
          
        
        (
        adj
        ⁡
        A
        
          )
          
            k
          
        
        .
      
    
    {\textstyle \bigwedge ^{n}(\operatorname {adj} A)^{k}.}
    Leverrier's Algorithm is an economical way of computing 
  
    
      
        
          ⋀
          
            n
          
        
        
          A
          
            k
          
        
      
    
    {\textstyle \bigwedge ^{n}A^{k}}
   and 
  
    
      
        
          ⋀
          
            n
            −
            1
          
        
        
          A
          
            k
          
        
        :
      
    
    {\textstyle \bigwedge ^{n-1}A^{k}\colon }
  

Set 
  
    
      
        
          ⋀
          
            n
            −
            1
          
        
        
          A
          
            0
          
        
        =
        1
        ;
      
    
    {\textstyle \bigwedge ^{n-1}A^{0}=1;}
  
For 
  
    
      
        k
        =
        n
        −
        1
        ,
        n
        −
        2
        ,
        …
        ,
        1
        ,
        0
        ,
      
    
    {\displaystyle k=n-1,n-2,\ldots ,1,0,}
  

  
    
      
        
          
            
              ⋀
            
          
          
            n
          
        
        
          A
          
            n
            −
            k
          
        
        =
        
          
            1
            
              n
              −
              k
            
          
        
        Tr
        ⁡
        (
        A
        ∘
        
          
            
              ⋀
            
          
          
            n
            −
            1
          
        
        
          A
          
            n
            −
            k
            −
            1
          
        
        )
        ;
      
    
    {\displaystyle {\textstyle \bigwedge }^{n}A^{n-k}={\frac {1}{n-k}}\operatorname {Tr} (A\circ {\textstyle \bigwedge }^{n-1}A^{n-k-1});}
  

  
    
      
        
          
            
              ⋀
            
          
          
            n
            −
            1
          
        
        
          A
          
            n
            −
            k
          
        
        =
        
          
            
              ⋀
            
          
          
            n
          
        
        
          A
          
            n
            −
            k
          
        
        ⋅
        Id
        −
        A
        ∘
        
          
            
              ⋀
            
          
          
            n
            −
            1
          
        
        
          A
          
            n
            −
            k
            −
            1
          
        
        .
      
    
    {\displaystyle {\textstyle \bigwedge }^{n-1}A^{n-k}={\textstyle \bigwedge }^{n}A^{n-k}\cdot \operatorname {Id} -A\circ {\textstyle \bigwedge }^{n-1}A^{n-k-1}.}

Physics
In physics, many quantities are naturally represented by alternating operators.  For example, if the motion of a charged particle is described by velocity and acceleration vectors in four-dimensional spacetime, then normalization of the velocity vector requires that the electromagnetic force must be an alternating operator on the velocity.  Its six degrees of freedom are identified with the electric and magnetic fields.

Linear geometry
The decomposable k-vectors have geometric interpretations: the bivector u ∧ v represents the plane spanned by the vectors, "weighted" with a number, given by the area of the oriented parallelogram with sides u and v.  Analogously, the 3-vector u ∧ v ∧ w represents the spanned 3-space weighted by the volume of the oriented parallelepiped with edges u, v, and w.

Projective geometry
Decomposable k-vectors in 
  
    
      
        
          ⋀
          
            k
          
        
        V
      
    
    {\textstyle \bigwedge \nolimits ^{k}V}
   correspond to weighted k-dimensional linear subspaces of V.  In particular, the Grassmannian of k-dimensional subspaces of V, denoted Grk(V), can be naturally identified with an algebraic subvariety of the projective space 
  
    
      
        P
        
          
            (
          
        
        
          ⋀
          
            k
          
        
        V
        
          
            )
          
        
        .
      
    
    {\textstyle P{\bigl (}\bigwedge \nolimits ^{k}V{\bigr )}.}
    This is called the Plücker embedding.

Differential geometry
The exterior algebra has notable applications in differential geometry, where it is used to define differential forms. Differential forms are mathematical objects that evaluate the length of vectors, areas of parallelograms, and volumes of higher-dimensional bodies, so they can be integrated over curves, surfaces and higher dimensional manifolds in a way that generalizes the line integrals and surface integrals from calculus.  A differential form at a point of a differentiable manifold is an alternating multilinear form on the tangent space at the point.  Equivalently, a differential form of degree k is a linear functional on the k-th exterior power of the tangent space.  As a consequence, the exterior product of multilinear forms defines a natural exterior product for differential forms.  Differential forms play a major role in diverse areas of differential geometry.
An alternate approach defines differential forms in terms of germs of functions.
In particular, the exterior derivative gives the exterior algebra of differential forms on a manifold the structure of a differential graded algebra.  The exterior derivative commutes with pullback along smooth mappings between manifolds, and it is therefore a natural differential operator.  The exterior algebra of differential forms, equipped with the exterior derivative, is a cochain complex whose cohomology is called the de Rham cohomology of the underlying manifold and plays a vital role in the algebraic topology of differentiable manifolds.

Representation theory
In representation theory, the exterior algebra is one of the two fundamental Schur functors on the category of vector spaces, the other being the symmetric algebra.  Together, these constructions are used to generate the irreducible representations of the general linear group; see fundamental representation.

Superspace
The exterior algebra over the complex numbers is the archetypal example of a superalgebra, which plays a fundamental role in physical theories pertaining to fermions and supersymmetry.  A single element of the exterior algebra is called a supernumber or Grassmann number.  The exterior algebra itself is then just a one-dimensional superspace: it is just the set of all of the points in the exterior algebra.  The topology on this space is essentially the weak topology, the open sets being the cylinder sets.  An n-dimensional superspace is just the n-fold product of exterior algebras.

Lie algebra homology
Let L be a Lie algebra over a field K, then it is possible to define the structure of a chain complex on the exterior algebra of L.  This is a K-linear mapping

  
    
      
        ∂
        :
        
          
            
              ⋀
            
          
          
            p
            +
            1
          
        
        L
        →
        
          
            
              ⋀
            
          
          
            p
          
        
        L
      
    
    {\displaystyle \partial :{\textstyle \bigwedge }^{p+1}L\to {\textstyle \bigwedge }^{p}L}
  defined on decomposable elements by

  
    
      
        ∂
        (
        
          x
          
            1
          
        
        ∧
        ⋯
        ∧
        
          x
          
            p
            +
            1
          
        
        )
        =
        
          
            1
            
              p
              +
              1
            
          
        
        
          ∑
          
            j
            <
            ℓ
          
        
        (
        −
        1
        
          )
          
            j
            +
            ℓ
            +
            1
          
        
        [
        
          x
          
            j
          
        
        ,
        
          x
          
            ℓ
          
        
        ]
        ∧
        
          x
          
            1
          
        
        ∧
        ⋯
        ∧
        
          
            
              
                x
                ^
              
            
          
          
            j
          
        
        ∧
        ⋯
        ∧
        
          
            
              
                x
                ^
              
            
          
          
            ℓ
          
        
        ∧
        ⋯
        ∧
        
          x
          
            p
            +
            1
          
        
        .
      
    
    {\displaystyle \partial (x_{1}\wedge \cdots \wedge x_{p+1})={\frac {1}{p+1}}\sum _{j<\ell }(-1)^{j+\ell +1}[x_{j},x_{\ell }]\wedge x_{1}\wedge \cdots \wedge {\hat {x}}_{j}\wedge \cdots \wedge {\hat {x}}_{\ell }\wedge \cdots \wedge x_{p+1}.}
  The Jacobi identity holds if and only if ∂∂ = 0, and so this is a necessary and sufficient condition for an anticommutative nonassociative algebra L to be a Lie algebra.  Moreover, in that case 
  
    
      
        ⋀
        L
      
    
    {\textstyle \bigwedge L}
   is a chain complex with boundary operator ∂.  The homology associated to this complex is the Lie algebra homology.

Homological algebra
The exterior algebra is the main ingredient in the construction of the Koszul complex, a fundamental object in homological algebra.

History
The exterior algebra was first introduced by Hermann Grassmann in 1844 under the blanket term of Ausdehnungslehre, or Theory of Extension.
This referred more generally to an algebraic (or axiomatic) theory of extended quantities and was one of the early precursors to the modern notion of a vector space.  Saint-Venant also published similar ideas of exterior calculus for which he claimed priority over Grassmann.The algebra itself was built from a set of rules, or axioms, capturing the formal aspects of Cayley and Sylvester's theory of multivectors.  It was thus a calculus, much like the propositional calculus, except focused exclusively on the task of formal reasoning in geometrical terms.
In particular, this new development allowed for an axiomatic characterization of dimension, a property that had previously only been examined from the coordinate point of view.
The import of this new theory of vectors and multivectors was lost to mid 19th century mathematicians,
until being thoroughly vetted by Giuseppe Peano in 1888.  Peano's work also remained somewhat obscure until the turn of the century, when the subject was unified by members of the French geometry school (notably Henri Poincaré, Élie Cartan, and Gaston Darboux) who applied Grassmann's ideas to the calculus of differential forms.
A short while later, Alfred North Whitehead, borrowing from the ideas of Peano and Grassmann, introduced his universal algebra.  This then paved the way for the 20th century developments of abstract algebra by placing the axiomatic notion of an algebraic system on a firm logical footing.

See also
Alternating algebra
Exterior calculus identities
Clifford algebra, a generalization of exterior algebra using a nonzero quadratic form
Geometric algebra
Koszul complex
Multilinear algebra
Symmetric algebra, the symmetric analog
Tensor algebra
Weyl algebra, a quantum deformation of the symmetric algebra by a symplectic form

Notes
References
Mathematical references
Bishop, R.; Goldberg, S.I. (1980), Tensor analysis on manifolds, Dover, ISBN 0-486-64039-6
Includes a treatment of alternating tensors and alternating forms, as well as a detailed discussion of Hodge duality from the perspective adopted in this article.
Bourbaki, Nicolas (1989), Elements of mathematics, Algebra I, Springer-Verlag, ISBN 3-540-64243-9
This is the main mathematical reference for the article.  It introduces the exterior algebra of a module over a commutative ring (although this article specializes primarily to the case when the ring is a field), including a discussion of the universal property, functoriality, duality, and the bialgebra structure.  See §III.7 and §III.11.
Bryant, R.L.; Chern, S.S.; Gardner, R.B.; Goldschmidt, H.L.; Griffiths, P.A. (1991), Exterior differential systems, Springer-Verlag
This book contains applications of exterior algebras to problems in partial differential equations.  Rank and related concepts are developed in the early chapters.
Mac Lane, S.; Birkhoff, G. (1999), Algebra, AMS Chelsea, ISBN 0-8218-1646-2
Chapter XVI sections 6–10 give a more elementary account of the exterior algebra, including duality, determinants and minors, and alternating forms.
Sternberg, Shlomo (1964), Lectures on Differential Geometry, Prentice Hall
Contains a classical treatment of the exterior algebra as alternating tensors, and applications to differential geometry.

Historical references
Bourbaki (1989, Historical note on chapters II and III)
Clifford, W. (1878), "Applications of Grassmann's Extensive Algebra", American Journal of Mathematics, The Johns Hopkins University Press, 1 (4): 350–358, doi:10.2307/2369379, JSTOR 2369379
Forder, H.G. (1941), The Calculus of Extension, Internet Archive
Grassmann, Hermann (1844), Die Lineale Ausdehnungslehre – Ein neuer Zweig der Mathematik (in German) (The Linear Extension Theory – A new Branch of Mathematics) alternative reference
Kannenberg, Lloyd (2000), Extension Theory (translation of Grassmann's Ausdehnungslehre), American Mathematical Society, ISBN 0-8218-2031-1
Peano, Giuseppe (1888), Calcolo Geometrico secondo l'Ausdehnungslehre di H. Grassmann preceduto dalle Operazioni della Logica Deduttiva; Kannenberg, Lloyd (1999), Geometric calculus: According to the Ausdehnungslehre of H. Grassmann, Birkhäuser, ISBN 978-0-8176-4126-9.
Whitehead, Alfred North (1898), "A Treatise on Universal Algebra, with Applications", Nature, Cambridge, 58 (1504): 385, Bibcode:1898Natur..58..385G, doi:10.1038/058385a0, S2CID 3985954

Other references and further reading
Browne, J.M. (2007), Grassmann algebra – Exploring applications of Extended Vector Algebra with Mathematica
An introduction to the exterior algebra, and geometric algebra, with a focus on applications.  Also includes a history section and bibliography.
Spivak, Michael (1965), Calculus on manifolds, Addison-Wesley, ISBN 978-0-8053-9021-6
Includes applications of the exterior algebra to differential forms, specifically focused on integration and Stokes's theorem.  The notation 
  
    
      
        
          ⋀
          
            k
          
        
        V
      
    
    {\textstyle \bigwedge \nolimits ^{k}V}
   in this text is used to mean the space of alternating k-forms on V; i.e., for Spivak 
  
    
      
        
          ⋀
          
            k
          
        
        V
      
    
    {\textstyle \bigwedge \nolimits ^{k}V}
   is what this article would call 
  
    
      
        
          ⋀
          
            k
          
        
        
          V
          
            ∗
          
        
        .
      
    
    {\textstyle \bigwedge \nolimits ^{k}V^{*}.}
    Spivak discusses this in Addendum 4.
Strang, G. (1993), Introduction to linear algebra, Wellesley-Cambridge Press, ISBN 978-0-9614088-5-5
Includes an elementary treatment of the axiomatization of determinants as signed areas, volumes, and higher-dimensional volumes.
Onishchik, A.L. (2001) [1994], "Exterior algebra", Encyclopedia of Mathematics, EMS Press
Wendell, Fleming (2012) [1977], "7. Exterior algebra and differential calculus", Functions of Several Variables (2nd ed.), Springer, pp. 275–320, ISBN 978-1-4684-9461-7
This textbook in multivariate calculus introduces the exterior algebra of differential forms adroitly into the calculus sequence for colleges.
Winitzki, S. (2010), Linear Algebra via Exterior Products
An introduction to the coordinate-free approach in basic finite-dimensional linear algebra, using exterior products.
Shafarevich, I.R.; Remizov, A.O. (2012). Linear Algebra and Geometry. Springer. ISBN 978-3-642-30993-9.
Chapter 10: The Exterior Product and Exterior Algebras
"The Grassmann method in projective geometry" A compilation of English translations of three notes by Cesare Burali-Forti on the application of exterior algebra to projective geometry
C. Burali-Forti, "Introduction to Differential Geometry, following the method of H. Grassmann" An English translation of an early book on the geometric applications of exterior algebras
"Mechanics, according to the principles of the theory of extension" An English translation of one Grassmann's papers on the applications of exterior algebra