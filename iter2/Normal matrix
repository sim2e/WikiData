In mathematics, a complex square matrix A is normal if it commutes with its conjugate transpose A*:

The concept of normal matrices can be extended to normal operators on infinite dimensional normed spaces and to normal elements in C*-algebras. As in the matrix case, normality means commutativity is preserved, to the extent possible, in the noncommutative setting. This makes normal operators, and normal elements of C*-algebras, more amenable to analysis.
The spectral theorem states that a matrix is normal if and only if it is unitarily similar to a diagonal matrix, and therefore any matrix A satisfying the equation A*A = AA* is  diagonalizable.  The converse does not hold because diagonalizable matrices may have non-orthogonal eigenspaces.
The left and right singular vectors in the singular value decomposition of a normal matrix 
  
    
      
        
          A
        
        =
        
          U
        
        
          Σ
        
        
          
            V
          
          
            ∗
          
        
      
    
    {\displaystyle \mathbf {A} =\mathbf {U} {\boldsymbol {\Sigma }}\mathbf {V} ^{*}}
   differ only in complex phase from each other and from the corresponding eigenvectors, since the phase must be factored out of the eigenvalues to form singular values.

Special cases
Among complex matrices, all unitary, Hermitian, and skew-Hermitian matrices are normal, with all eigenvalues being unit modulus, real, and imaginary, respectively. Likewise, among real matrices, all orthogonal, symmetric, and skew-symmetric matrices are normal, with all eigenvalues being complex conjugate pairs on the unit circle, real, and imaginary, respectively. However, it is not the case that all normal matrices are either unitary or (skew-)Hermitian, as their eigenvalues can be any complex number, in general.  For example,

is neither unitary, Hermitian, nor skew-Hermitian, because its eigenvalues are 
  
    
      
        2
        ,
        (
        1
        ±
        i
        
          
            3
          
        
        )
        
          /
        
        2
      
    
    {\displaystyle 2,(1\pm i{\sqrt {3}})/2}
  ; yet it is normal because

Consequences
The concept of normality is important because normal matrices are precisely those to which the spectral theorem applies: 

The diagonal entries of Λ are the eigenvalues of A, and the columns of U are the eigenvectors of A. The matching eigenvalues in Λ come in the same order as the eigenvectors are ordered as columns of U.
Another way of stating the spectral theorem is to say that normal matrices are precisely those matrices that can be represented by a diagonal matrix with respect to a properly chosen orthonormal basis of Cn. Phrased differently: a matrix is normal if and only if its eigenspaces span Cn and are pairwise orthogonal with respect to the standard inner product of Cn.
The spectral theorem for normal matrices is a special case of the more general Schur decomposition which holds for all square matrices. Let A be a square matrix. Then by Schur decomposition it is unitary similar to an upper-triangular matrix, say, B. If A is normal, so is B. But then B must be diagonal, for, as noted above, a normal upper-triangular matrix is diagonal.
The spectral theorem permits the classification of normal matrices in terms of their spectra, for example: 

In general, the sum or product of two normal matrices need not be normal. However, the following holds: 

In this special case, the columns of U* are eigenvectors of both A and B and form an orthonormal basis in Cn. This follows by combining the theorems that, over an algebraically closed field, commuting matrices are simultaneously triangularizable and a normal matrix is diagonalizable – the added result is that these can both be done simultaneously.

Equivalent definitions
It is possible to give a fairly long list of equivalent definitions of a normal matrix. Let A be a n × n complex matrix. Then the following are equivalent:

A is normal.
A is diagonalizable by a unitary matrix.
There exists a set of eigenvectors of A which forms an orthonormal basis for Cn.

  
    
      
        
          ‖
          
            A
            
              x
            
          
          ‖
        
        =
        
          ‖
          
            
              A
              
                ∗
              
            
            
              x
            
          
          ‖
        
      
    
    {\displaystyle \left\|A\mathbf {x} \right\|=\left\|A^{*}\mathbf {x} \right\|}
   for every x.
The Frobenius norm of A can be computed by the eigenvalues of A: 
  
    
      
        tr
        ⁡
        
          (
          
            
              A
              
                ∗
              
            
            A
          
          )
        
        =
        
          ∑
          
            j
          
        
        
          
            |
            
              λ
              
                j
              
            
            |
          
          
            2
          
        
      
    
    {\textstyle \operatorname {tr} \left(A^{*}A\right)=\sum _{j}\left|\lambda _{j}\right|^{2}}
  .
The Hermitian part 1/2(A + A*) and skew-Hermitian part 1/2(A − A*) of A commute.
A* is a polynomial (of degree ≤ n − 1) in A.
A* = AU for some unitary matrix U.
U and P commute, where we have the polar decomposition A = UP with a unitary matrix U and some positive semidefinite matrix P.
A commutes with some normal matrix N with distinct eigenvalues.
σi = |λi| for all 1 ≤ i ≤ n where A has singular values σ1 ≥ ⋯ ≥ σn and has eigenvalues that are indexed with ordering |λ1| ≥ ⋯ ≥ |λn|.Some but not all of the above generalize to normal operators on infinite-dimensional Hilbert spaces. For example, a bounded operator satisfying (9) is only quasinormal.

Normal matrix analogy
It is occasionally useful (but sometimes misleading) to think of the relationships of special kinds of normal matrices as analogous to the relationships of the corresponding type of complex numbers of which their eigenvalues are composed.  This is because any function of a non-defective matrix acts directly on each of its eigenvalues, and the conjugate transpose of its spectral decomposition 
  
    
      
        V
        D
        
          V
          
            ∗
          
        
      
    
    {\displaystyle VDV^{*}}
   is 
  
    
      
        V
        
          D
          
            ∗
          
        
        
          V
          
            ∗
          
        
      
    
    {\displaystyle VD^{*}V^{*}}
  , where 
  
    
      
        D
      
    
    {\displaystyle D}
   is the diagonal matrix of eigenvalues.  Likewise, if two normal matrices commute and are therefore simultaneously diagonalizable, any operation between these matrices also acts on each corresponding pair of eigenvalues.

The conjugate transpose is analogous to the complex conjugate.
Unitary matrices are analogous to complex numbers on the unit circle.
Hermitian matrices are analogous to real numbers.
Hermitian positive definite matrices are analogous to positive real numbers.
Skew Hermitian matrices are analogous to purely imaginary numbers.
Invertible matrices are analogous to non-zero complex numbers.
The inverse of a matrix has each eigenvalue inverted.
A uniform scaling matrix is analogous to a constant number.
In particular, the zero is analogous to 0, and
the identity matrix is analogous to 1.
An idempotent matrix is an orthogonal projection with each eigenvalue either 0 or 1.
A normal involution has eigenvalues 
  
    
      
        ±
        1
      
    
    {\displaystyle \pm 1}
  .As a special case, the complex numbers may be embedded in the normal 2×2 real matrices by the mapping 

which preserves addition and multiplication. It is easy to check that this embedding respects all of the above analogies.

See also
Hermitian matrix
Least-squares normal matrix

Notes
Citations
Sources
Horn, Roger Alan; Johnson, Charles Royal (1985), Matrix Analysis, Cambridge University Press, ISBN 978-0-521-38632-6.
Horn, Roger Alan; Johnson, Charles Royal (1991). Topics in Matrix Analysis. Cambridge University Press. ISBN 978-0-521-30587-7.