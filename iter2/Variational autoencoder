In machine learning, a variational autoencoder (VAE) is an artificial neural network architecture introduced by Diederik P. Kingma and Max Welling. It is part of the families of probabilistic graphical models and variational Bayesian methods.Variational autoencoders are often associated with the autoencoder model because of its architectural affinity, but with significant differences in the goal and mathematical formulation. Variational autoencoders are probabilistic generative models that require neural networks as only a part of their overall structure. The neural network components are typically referred to as the encoder and decoder for the first and second component respectively. The first neural network maps the input variable to a latent space that corresponds to the parameters of a variational distribution. In this way, the encoder can produce multiple different samples that all come from the same distribution. The decoder has the opposite function, which is to map from the latent space to the input space, in order to produce or generate data points. Both networks are typically trained together with the usage of the reparameterization trick, although the variance of the noise model can be learned separately.
Although this type of model was initially designed for unsupervised learning, its effectiveness has been proven for semi-supervised learning and supervised learning.

Overview of architecture and operation
A variational autoencoder is a generative model with a prior and noise distribution respectively. Usually such models are trained using the expectation-maximization meta-algorithm (e.g. probabilistic PCA, (spike & slab) sparse coding). Such a scheme optimizes a lower bound of the data likelihood, which is usually intractable, and in doing so requires the discovery of q-distributions, or variational posteriors. These q-distributions are normally parameterized for each individual data point in a separate optimization process. However, variational autoencoders use a neural network as an amortized approach to jointly optimize across data points. This neural network takes as input the data points themselves, and outputs parameters for the variational distribution. As it maps from a known input space to the low-dimensional latent space, it is called the encoder.
The decoder is the second neural network of this model. It is a function that maps from the latent space to the input space, e.g. as the means of the noise distribution. It is possible to use another neural network that maps to the variance, however this can be omitted for simplicity. In such a case, the variance can be optimized with gradient descent.
To optimize this model, one needs to know two terms: the "reconstruction error", and the Kullback–Leibler divergence. Both terms are derived from the free energy expression of the probabilistic model, and therefore differ depending on the noise distribution and the assumed prior of the data. For example, a standard VAE task such as IMAGENET is typically assumed to have a gaussianly distributed noise, however tasks such as binarized MNIST require a Bernoulli noise. The KL-D from the free energy expression maximizes the probability mass of the q-distribution that overlaps with the p-distribution, which unfortunately can result in mode-seeking behaviour. The "reconstruction" term is the remainder of the free energy expression, and requires a sampling approximation to compute its expectation value.

Formulation
From the point of view of probabilistic modeling, one wants to maximize the likelihood of the data 
  
    
      
        x
      
    
    {\displaystyle x}
   by their chosen parameterized probability distribution 
  
    
      
        
          p
          
            θ
          
        
        (
        x
        )
        =
        p
        (
        x
        
          |
        
        θ
        )
      
    
    {\displaystyle p_{\theta }(x)=p(x|\theta )}
  . This distribution is usually chosen to be a Gaussian 
  
    
      
        N
        (
        x
        
          |
        
        μ
        ,
        σ
        )
      
    
    {\displaystyle N(x|\mu ,\sigma )}
   which is parameterized by 
  
    
      
        μ
      
    
    {\displaystyle \mu }
   and 
  
    
      
        σ
      
    
    {\displaystyle \sigma }
   respectively, and as a member of the exponential family it is easy to work with as a noise distribution. Simple distributions are easy enough to maximize, however distributions where a prior is assumed over the latents 
  
    
      
        z
      
    
    {\displaystyle z}
   results in intractable integrals. Let us find 
  
    
      
        
          p
          
            θ
          
        
        (
        x
        )
      
    
    {\displaystyle p_{\theta }(x)}
   via marginalizing over 
  
    
      
        z
      
    
    {\displaystyle z}
  .

  
    
      
        
          p
          
            θ
          
        
        (
        x
        )
        =
        
          ∫
          
            z
          
        
        
          p
          
            θ
          
        
        (
        
          x
          ,
          z
        
        )
        
        d
        z
        ,
      
    
    {\displaystyle p_{\theta }(x)=\int _{z}p_{\theta }({x,z})\,dz,}
  where 
  
    
      
        
          p
          
            θ
          
        
        (
        
          x
          ,
          z
        
        )
      
    
    {\displaystyle p_{\theta }({x,z})}
   represents the joint distribution under 
  
    
      
        
          p
          
            θ
          
        
      
    
    {\displaystyle p_{\theta }}
   of the observable data 
  
    
      
        x
      
    
    {\displaystyle x}
   and its latent representation or encoding 
  
    
      
        z
      
    
    {\displaystyle z}
  . According to the chain rule, the equation can be rewritten as

  
    
      
        
          p
          
            θ
          
        
        (
        x
        )
        =
        
          ∫
          
            z
          
        
        
          p
          
            θ
          
        
        (
        
          x
          
            |
          
          z
        
        )
        
          p
          
            θ
          
        
        (
        z
        )
        
        d
        z
      
    
    {\displaystyle p_{\theta }(x)=\int _{z}p_{\theta }({x|z})p_{\theta }(z)\,dz}
  In the vanilla variational autoencoder, 
  
    
      
        z
      
    
    {\displaystyle z}
   is usually taken to be a finite-dimensional vector of real numbers, and 
  
    
      
        
          p
          
            θ
          
        
        (
        
          x
          
            |
          
          z
        
        )
      
    
    {\displaystyle p_{\theta }({x|z})}
   to be a Gaussian distribution. Then 
  
    
      
        
          p
          
            θ
          
        
        (
        x
        )
      
    
    {\displaystyle p_{\theta }(x)}
   is a mixture of Gaussian distributions.
It is now possible to define the set of the relationships between the input data and its latent representation as

Prior 
  
    
      
        
          p
          
            θ
          
        
        (
        z
        )
      
    
    {\displaystyle p_{\theta }(z)}
  
Likelihood 
  
    
      
        
          p
          
            θ
          
        
        (
        x
        
          |
        
        z
        )
      
    
    {\displaystyle p_{\theta }(x|z)}
  
Posterior 
  
    
      
        
          p
          
            θ
          
        
        (
        z
        
          |
        
        x
        )
      
    
    {\displaystyle p_{\theta }(z|x)}
  Unfortunately, the computation of 
  
    
      
        
          p
          
            θ
          
        
        (
        z
        
          |
        
        x
        )
      
    
    {\displaystyle p_{\theta }(z|x)}
   is expensive and in most cases intractable. To speed up the calculus to make it feasible, it is necessary to introduce a further function to approximate the posterior distribution as

  
    
      
        
          q
          
            ϕ
          
        
        (
        
          z
          
            |
          
          x
        
        )
        ≈
        
          p
          
            θ
          
        
        (
        
          z
          
            |
          
          x
        
        )
      
    
    {\displaystyle q_{\phi }({z|x})\approx p_{\theta }({z|x})}
  with 
  
    
      
        ϕ
      
    
    {\displaystyle \phi }
   defined as the set of real values that parametrize 
  
    
      
        q
      
    
    {\displaystyle q}
  . This is sometimes called amortized inference, since by "investing" in finding a good 
  
    
      
        
          q
          
            ϕ
          
        
      
    
    {\displaystyle q_{\phi }}
  , one can later infer 
  
    
      
        z
      
    
    {\displaystyle z}
   from 
  
    
      
        x
      
    
    {\displaystyle x}
   quickly without doing any integrals.
In this way, the problem is to find a good probabilistic autoencoder, in which the conditional likelihood distribution 
  
    
      
        
          p
          
            θ
          
        
        (
        x
        
          |
        
        z
        )
      
    
    {\displaystyle p_{\theta }(x|z)}
   is computed by the probabilistic decoder, and the approximated posterior distribution 
  
    
      
        
          q
          
            ϕ
          
        
        (
        z
        
          |
        
        x
        )
      
    
    {\displaystyle q_{\phi }(z|x)}
   is computed by the probabilistic encoder.
Parametrize the encoder as 
  
    
      
        
          E
          
            ϕ
          
        
      
    
    {\displaystyle E_{\phi }}
  , and the decoder as 
  
    
      
        
          D
          
            θ
          
        
      
    
    {\displaystyle D_{\theta }}
  .

Evidence lower bound (ELBO)
As in every deep learning problem, it is necessary to define a differentiable loss function in order to update the network weights through backpropagation.
For variational autoencoders, the idea is to jointly optimize the generative model parameters 
  
    
      
        θ
      
    
    {\displaystyle \theta }
   to reduce the reconstruction error between the input and the output, and 
  
    
      
        ϕ
      
    
    {\displaystyle \phi }
   to make 
  
    
      
        
          q
          
            ϕ
          
        
        (
        
          z
          
            |
          
          x
        
        )
      
    
    {\displaystyle q_{\phi }({z|x})}
   as close as possible to 
  
    
      
        
          p
          
            θ
          
        
        (
        z
        
          |
        
        x
        )
      
    
    {\displaystyle p_{\theta }(z|x)}
  . As reconstruction loss, mean squared error and cross entropy are often used.
As distance loss between the two distributions the Kullback–Leibler divergence 
  
    
      
        
          D
          
            K
            L
          
        
        (
        
          q
          
            ϕ
          
        
        (
        
          z
          
            |
          
          x
        
        )
        ∥
        
          p
          
            θ
          
        
        (
        
          z
          
            |
          
          x
        
        )
        )
      
    
    {\displaystyle D_{KL}(q_{\phi }({z|x})\parallel p_{\theta }({z|x}))}
   is a good choice to squeeze 
  
    
      
        
          q
          
            ϕ
          
        
        (
        
          z
          
            |
          
          x
        
        )
      
    
    {\displaystyle q_{\phi }({z|x})}
   under 
  
    
      
        
          p
          
            θ
          
        
        (
        z
        
          |
        
        x
        )
      
    
    {\displaystyle p_{\theta }(z|x)}
  .The distance loss just defined is expanded as

  
    
      
        
          
            
              
                
                  D
                  
                    K
                    L
                  
                
                (
                
                  q
                  
                    ϕ
                  
                
                (
                
                  z
                  
                    |
                  
                  x
                
                )
                ∥
                
                  p
                  
                    θ
                  
                
                (
                
                  z
                  
                    |
                  
                  x
                
                )
                )
              
              
                
                =
                
                  
                    E
                  
                  
                    z
                    ∼
                    
                      q
                      
                        ϕ
                      
                    
                    (
                    ⋅
                    
                      |
                    
                    x
                    )
                  
                
                
                  [
                  
                    ln
                    ⁡
                    
                      
                        
                          
                            q
                            
                              ϕ
                            
                          
                          (
                          z
                          
                            |
                          
                          x
                          )
                        
                        
                          
                            p
                            
                              θ
                            
                          
                          (
                          z
                          
                            |
                          
                          x
                          )
                        
                      
                    
                  
                  ]
                
              
            
            
              
              
                
                =
                
                  
                    E
                  
                  
                    z
                    ∼
                    
                      q
                      
                        ϕ
                      
                    
                    (
                    ⋅
                    
                      |
                    
                    x
                    )
                  
                
                
                  [
                  
                    ln
                    ⁡
                    
                      
                        
                          
                            q
                            
                              ϕ
                            
                          
                          (
                          
                            z
                            
                              |
                            
                            x
                          
                          )
                          
                            p
                            
                              θ
                            
                          
                          (
                          x
                          )
                        
                        
                          
                            p
                            
                              θ
                            
                          
                          (
                          x
                          ,
                          z
                          )
                        
                      
                    
                  
                  ]
                
              
            
            
              
              
                
                =
                ln
                ⁡
                
                  p
                  
                    θ
                  
                
                (
                x
                )
                +
                
                  
                    E
                  
                  
                    z
                    ∼
                    
                      q
                      
                        ϕ
                      
                    
                    (
                    ⋅
                    
                      |
                    
                    x
                    )
                  
                
                
                  [
                  
                    ln
                    ⁡
                    
                      
                        
                          
                            q
                            
                              ϕ
                            
                          
                          (
                          
                            z
                            
                              |
                            
                            x
                          
                          )
                        
                        
                          
                            p
                            
                              θ
                            
                          
                          (
                          x
                          ,
                          z
                          )
                        
                      
                    
                  
                  ]
                
              
            
          
        
      
    
    {\displaystyle {\begin{aligned}D_{KL}(q_{\phi }({z|x})\parallel p_{\theta }({z|x}))&=\mathbb {E} _{z\sim q_{\phi }(\cdot |x)}\left[\ln {\frac {q_{\phi }(z|x)}{p_{\theta }(z|x)}}\right]\\&=\mathbb {E} _{z\sim q_{\phi }(\cdot |x)}\left[\ln {\frac {q_{\phi }({z|x})p_{\theta }(x)}{p_{\theta }(x,z)}}\right]\\&=\ln p_{\theta }(x)+\mathbb {E} _{z\sim q_{\phi }(\cdot |x)}\left[\ln {\frac {q_{\phi }({z|x})}{p_{\theta }(x,z)}}\right]\end{aligned}}}
  Now define the evidence lower bound (ELBO):Maximizing the ELBOis equivalent to simultaneously maximizing 
  
    
      
        ln
        ⁡
        
          p
          
            θ
          
        
        (
        x
        )
      
    
    {\displaystyle \ln p_{\theta }(x)}
   and minimizing 
  
    
      
        
          D
          
            K
            L
          
        
        (
        
          q
          
            ϕ
          
        
        (
        
          z
          
            |
          
          x
        
        )
        ∥
        
          p
          
            θ
          
        
        (
        
          z
          
            |
          
          x
        
        )
        )
      
    
    {\displaystyle D_{KL}(q_{\phi }({z|x})\parallel p_{\theta }({z|x}))}
  . That is, maximizing the log-likelihood of the observed data, and minimizing the divergence of the approximate posterior 
  
    
      
        
          q
          
            ϕ
          
        
        (
        ⋅
        
          |
        
        x
        )
      
    
    {\displaystyle q_{\phi }(\cdot |x)}
   from the exact posterior 
  
    
      
        
          p
          
            θ
          
        
        (
        ⋅
        
          |
        
        x
        )
      
    
    {\displaystyle p_{\theta }(\cdot |x)}
  .
The form given is not very convenient for maximization, but the following, equivalent form, is:where 
  
    
      
        ln
        ⁡
        
          p
          
            θ
          
        
        (
        x
        
          |
        
        z
        )
      
    
    {\displaystyle \ln p_{\theta }(x|z)}
   is implemented as 
  
    
      
        −
        
          
            1
            2
          
        
        ‖
        x
        −
        
          D
          
            θ
          
        
        (
        z
        )
        
          ‖
          
            2
          
          
            2
          
        
      
    
    {\displaystyle -{\frac {1}{2}}\|x-D_{\theta }(z)\|_{2}^{2}}
  , since that is, up to an additive constant, what 
  
    
      
        x
        ∼
        
          
            N
          
        
        (
        
          D
          
            θ
          
        
        (
        z
        )
        ,
        I
        )
      
    
    {\displaystyle x\sim {\mathcal {N}}(D_{\theta }(z),I)}
   yields. That is, we model the distribution of 
  
    
      
        x
      
    
    {\displaystyle x}
   conditional on 
  
    
      
        z
      
    
    {\displaystyle z}
   to be a Gaussian distribution centered on 
  
    
      
        
          D
          
            θ
          
        
        (
        z
        )
      
    
    {\displaystyle D_{\theta }(z)}
  . The distribution of 
  
    
      
        
          q
          
            ϕ
          
        
        (
        z
        
          |
        
        x
        )
      
    
    {\displaystyle q_{\phi }(z|x)}
   and 
  
    
      
        
          p
          
            θ
          
        
        (
        z
        )
      
    
    {\displaystyle p_{\theta }(z)}
   are often also chosen to be Gaussians as 
  
    
      
        z
        
          |
        
        x
        ∼
        
          
            N
          
        
        (
        
          E
          
            ϕ
          
        
        (
        x
        )
        ,
        
          σ
          
            ϕ
          
        
        (
        x
        
          )
          
            2
          
        
        I
        )
      
    
    {\displaystyle z|x\sim {\mathcal {N}}(E_{\phi }(x),\sigma _{\phi }(x)^{2}I)}
   and 
  
    
      
        z
        ∼
        
          
            N
          
        
        (
        0
        ,
        I
        )
      
    
    {\displaystyle z\sim {\mathcal {N}}(0,I)}
  , with which we obtain by the formula for KL divergence of Gaussians:Here 
  
    
      
        N
      
    
    {\displaystyle N}
   is the dimension of 
  
    
      
        z
      
    
    {\displaystyle z}
  . For a more detailed derivation and more interpretations of ELBO and its maximization, see its main page.

Reparameterization
To efficiently search for the typical method is gradient descent.
It is straightforward to findHowever, does not allow one to put the 
  
    
      
        
          ∇
          
            ϕ
          
        
      
    
    {\displaystyle \nabla _{\phi }}
   inside the expectation, since 
  
    
      
        ϕ
      
    
    {\displaystyle \phi }
   appears in the probability distribution itself. The reparameterization trick (also known as stochastic backpropagation) bypasses this difficulty.The most important example is when 
  
    
      
        z
        ∼
        
          q
          
            ϕ
          
        
        (
        ⋅
        
          |
        
        x
        )
      
    
    {\displaystyle z\sim q_{\phi }(\cdot |x)}
   is normally distributed, as 
  
    
      
        
          
            N
          
        
        (
        
          μ
          
            ϕ
          
        
        (
        x
        )
        ,
        
          Σ
          
            ϕ
          
        
        (
        x
        )
        )
      
    
    {\displaystyle {\mathcal {N}}(\mu _{\phi }(x),\Sigma _{\phi }(x))}
  .

This can be reparametrized by letting 
  
    
      
        
          ε
        
        ∼
        
          
            N
          
        
        (
        0
        ,
        
          I
        
        )
      
    
    {\displaystyle {\boldsymbol {\varepsilon }}\sim {\mathcal {N}}(0,{\boldsymbol {I}})}
   be a "standard random number generator", and construct 
  
    
      
        z
      
    
    {\displaystyle z}
   as 
  
    
      
        z
        =
        
          μ
          
            ϕ
          
        
        (
        x
        )
        +
        
          L
          
            ϕ
          
        
        (
        x
        )
        ϵ
      
    
    {\displaystyle z=\mu _{\phi }(x)+L_{\phi }(x)\epsilon }
  . Here, 
  
    
      
        
          L
          
            ϕ
          
        
        (
        x
        )
      
    
    {\displaystyle L_{\phi }(x)}
   is obtained by the Cholesky decomposition:Then we haveand so we obtained an unbiased estimator of the gradient, allowing stochastic gradient descent.
Since we reparametrized 
  
    
      
        z
      
    
    {\displaystyle z}
  , we need to find 
  
    
      
        
          q
          
            ϕ
          
        
        (
        z
        
          |
        
        x
        )
      
    
    {\displaystyle q_{\phi }(z|x)}
  . Let 
  
    
      
        
          q
          
            0
          
        
      
    
    {\displaystyle q_{0}}
   by the probability density function for 
  
    
      
        ϵ
      
    
    {\displaystyle \epsilon }
  , thenwhere 
  
    
      
        
          ∂
          
            ϵ
          
        
        z
      
    
    {\displaystyle \partial _{\epsilon }z}
   is the Jacobian matrix of 
  
    
      
        ϵ
      
    
    {\displaystyle \epsilon }
   with respect to 
  
    
      
        z
      
    
    {\displaystyle z}
  . Since 
  
    
      
        z
        =
        
          μ
          
            ϕ
          
        
        (
        x
        )
        +
        
          L
          
            ϕ
          
        
        (
        x
        )
        ϵ
      
    
    {\displaystyle z=\mu _{\phi }(x)+L_{\phi }(x)\epsilon }
  , this is

Variations
Many variational autoencoders applications and extensions have been used to adapt the architecture to other domains and improve its performance. 

  
    
      
        β
      
    
    {\displaystyle \beta }
  -VAE is an implementation with a weighted Kullback–Leibler divergence term to automatically discover and interpret factorised latent representations. With this implementation, it is possible to force manifold disentanglement for 
  
    
      
        β
      
    
    {\displaystyle \beta }
   values greater than one. This architecture can discover disentangled latent factors without supervision.The conditional VAE (CVAE), inserts label information in the latent space to force a deterministic constrained representation of the learned data.Some structures directly deal with the quality of the generated samples or implement more than one latent space to further improve the representation learning.
Some architectures mix VAE and generative adversarial networks to obtain hybrid models.

See also


== References ==